{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: blue; text-align: center; font-size: 32px; font-weight: bold;\">\n",
    "  üöÄüöÄ ¬°Backpropagation, Hopfield Neural Network, Physics Informed Neural Network!üöÄüöÄ\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"back.png\" width=\"4000\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "\n",
    "1. [Introducci√≥n](#introduction)\n",
    "2. [Algoritmo de premio nobel](#introduction)\n",
    "   - 2.1 [Teor√≠a matem√°tica del backpropagation](#basic-concepts)\n",
    "   \n",
    "4. [Red Neuronal del premio nobel](#applications-of-cnns)\n",
    "   - 4.1 [Hopfield Neural Network](#convolution)\n",
    "   - 4.2 [Boltzman Machine](#activation-functions)\n",
    "   \n",
    "5. [Pysics Informed Neural Network](#convolution)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. [Introducci√≥n](#introduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes Neuronales: Backpropagation, Hopfield Networks y Physics-Informed Neural Networks (PINNs)\n",
    "\n",
    "## Introducci√≥n\n",
    "\n",
    "En este notebook se exploran tres enfoques relevantes dentro del estudio de las redes neuronales, tanto desde la perspectiva hist√≥rica como desde sus aplicaciones modernas en matem√°ticas aplicadas y f√≠sica computacional.\n",
    "\n",
    "### 1Ô∏è‚É£ Backpropagation (Retropropagaci√≥n del Error)\n",
    "El algoritmo de **backpropagation** es el pilar fundamental en el entrenamiento de redes neuronales profundas. Permite optimizar los pesos de la red mediante el c√°lculo eficiente del gradiente a trav√©s de la **regla de la cadena** del c√°lculo diferencial. Gracias a este algoritmo, hoy es posible entrenar redes complejas como CNNs y Transformers.  \n",
    "En esta secci√≥n revisaremos:\n",
    "- Su formulaci√≥n matem√°tica.\n",
    "- Su implementaci√≥n pr√°ctica.\n",
    "- Ejemplos de redes neuronales simples entrenadas por este m√©todo.\n",
    "\n",
    "---\n",
    "\n",
    "### 2Ô∏è‚É£ Redes de Hopfield (Hopfield Networks)\n",
    "Las **redes de Hopfield** representan un modelo cl√°sico de redes neuronales recurrentes, donde el principio de minimizaci√≥n de energ√≠a es central. Estas redes permiten almacenar patrones y recuperarlos a partir de entradas incompletas o ruidosas, actuando como modelos de **memoria asociativa**. Aunque son redes m√°s antiguas, tienen conexiones profundas con la f√≠sica estad√≠stica (funci√≥n energ√≠a, redes de Ising) y la optimizaci√≥n.\n",
    "En esta secci√≥n veremos:\n",
    "- El funcionamiento de las redes de Hopfield.\n",
    "- Su formulaci√≥n como sistemas din√°micos.\n",
    "- Ejemplos de recuperaci√≥n de patrones.\n",
    "\n",
    "---\n",
    "\n",
    "### 3Ô∏è‚É£ Physics-Informed Neural Networks (PINNs)\n",
    "Las **Physics-Informed Neural Networks (PINNs)** son un desarrollo reciente que permite integrar **ecuaciones diferenciales** (PDEs, ODEs) directamente en el entrenamiento de una red neuronal. A diferencia de los enfoques puramente basados en datos, las PINNs utilizan el conocimiento f√≠sico como una parte expl√≠cita de la funci√≥n de p√©rdida, lo que las hace muy √∫tiles para problemas inversos y mal planteados en f√≠sica, ingenier√≠a y finanzas.\n",
    "En esta secci√≥n abordaremos:\n",
    "- El principio de funcionamiento de las PINNs.\n",
    "- C√≥mo se integran las condiciones de frontera y las ecuaciones diferenciales.\n",
    "- Ejemplos pr√°cticos de uso.\n",
    "\n",
    "---\n",
    "\n",
    "## Objetivo del Notebook\n",
    "Al finalizar este notebook, el lector ser√° capaz de:\n",
    "- Entender y aplicar el algoritmo de backpropagation.\n",
    "- Comprender el funcionamiento y las aplicaciones de las redes de Hopfield.\n",
    "- Reconocer el potencial y las aplicaciones de las PINNs en la resoluci√≥n de problemas f√≠sicos.\n",
    "\n",
    "Cada secci√≥n incluir√° teor√≠a, ejemplos y c√≥digo para reforzar el aprendizaje.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. [\"Algoritmo de premio nobel\"](#introduction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El algoritmo de **backpropagation** (retropropagaci√≥n del error) es el m√©todo fundamental que permiti√≥ el desarrollo pr√°ctico del aprendizaje profundo. Aunque las bases del algoritmo datan de las d√©cadas de 1960 y 1970 (Bryson & Ho, 1969; Werbos, 1974), no fue hasta 1986 cuando el trabajo de **Rumelhart, Hinton y Williams** lo populariz√≥ como un m√©todo eficiente para entrenar redes neuronales multicapa (MLP) utilizando la regla de la cadena para propagar los gradientes desde la salida hacia las capas internas (Rumelhart et al., 1986).\n",
    "\n",
    "### ¬øQu√© es Backpropagation?\n",
    "Es un algoritmo basado en **gradiente descendente** que ajusta los pesos de una red neuronal para minimizar una funci√≥n de p√©rdida. Consiste en dos fases:\n",
    "1. **Propagaci√≥n hacia adelante** (Forward Pass): Se calcula la salida de la red.\n",
    "2. **Propagaci√≥n hacia atr√°s** (Backward Pass): Se calcula el gradiente de la p√©rdida respecto a cada peso usando la regla de la cadena.\n",
    "\n",
    "Gracias a esta t√©cnica, fue posible entrenar redes profundas, marcando el inicio de la era moderna del aprendizaje profundo.\n",
    "\n",
    "### Referencias Bibliogr√°ficas\n",
    "- Rumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986). *Learning representations by back-propagating errors*. Nature, 323(6088), 533‚Äì536. [https://doi.org/10.1038/323533a0](https://doi.org/10.1038/323533a0)\n",
    "- Werbos, P. J. (1974). *Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences* (Doctoral dissertation, Harvard University).\n",
    "- Bryson, A. E., & Ho, Y.-C. (1969). *Applied Optimal Control: Optimization, Estimation, and Control*. Blaisdell Publishing Company.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# - 2.1 [Teor√≠a matem√°tica del backpropagation](#basic-concepts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"eje.png\" width=\"4000\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conceptos Clave de las Redes Neuronales\n",
    "\n",
    "1. **Neurona**:  \n",
    "   Es la unidad fundamental de c√≥mputo en una red neuronal. Recibe se√±ales de entrada, las procesa y produce una se√±al de salida.\n",
    "\n",
    "2. **Capa de Entrada**:  \n",
    "   Es la capa de neuronas que recibe los datos de entrada del mundo exterior o de otra capa de la red. Cada neurona en la capa de entrada corresponde a una caracter√≠stica (feature) del conjunto de datos.\n",
    "\n",
    "3. **Capa Oculta**:  \n",
    "   Son una o m√°s capas de neuronas situadas entre la capa de entrada y la de salida. Estas capas son responsables de aprender representaciones del conjunto de datos. Transforman las entradas mediante conexiones ponderadas y funciones de activaci√≥n.\n",
    "\n",
    "4. **Capa de Salida**:  \n",
    "   Es la capa de neuronas que genera la salida final de la red neuronal. El n√∫mero de neuronas en la capa de salida depende de la naturaleza de la tarea para la cual fue dise√±ada la red. Por ejemplo, en una tarea de clasificaci√≥n con tres clases, normalmente habr√° tres neuronas en la capa de salida.\n",
    "\n",
    "5. **Pesos de Conexi√≥n**:  \n",
    "   Son los valores que determinan la fuerza de las conexiones entre neuronas de capas adyacentes. Estos pesos controlan cu√°nto influye una neurona sobre otra. Durante el entrenamiento, la red ajusta estos pesos para aprender a partir de los datos de entrada y la salida deseada.\n",
    "\n",
    "6. **Funci√≥n de Activaci√≥n**:  \n",
    "   Es una funci√≥n que se aplica a la suma ponderada de las entradas para introducir no linealidad en la salida de una neurona. Las funciones de activaci√≥n m√°s comunes incluyen sigmoide, tangente hiperb√≥lica (tanh), ReLU (Unidad Lineal Rectificada) y softmax. Estas funciones permiten que la red aprenda patrones y relaciones complejas en los datos.\n",
    "\n",
    "7. **Sesgo (Bias)**:  \n",
    "   Es un par√°metro adicional asociado a cada neurona de una capa que permite a la red ajustarse mejor a los datos. El sesgo desplaza la funci√≥n de activaci√≥n, influyendo en el punto donde una neurona comienza a activarse. Esto ayuda a la red a aprender patrones que no necesariamente pasan por el origen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Terminolog√≠a\n",
    "\n",
    "$a^k_j$ indica la neurona en la capa $k$ con √≠ndice $j$, empezando en $1$ desde arriba.\n",
    "\n",
    "$w^k_{ij}$ indica el peso de la capa $k$ que conecta la neurona $a^{k-1}_i$ con la neurona $a^{k}_j$.\n",
    "\n",
    "$b^k_j$ indica el sesgo (bias) en la capa $k$ con √≠ndice $j$, empezando en $1$ desde arriba.\n",
    "\n",
    "$z^k_j = a^{k-1}_{1} w^{k}_{1j} + a^{k-1}_{2} w^{k}_{2j} + a^{k-1}_{3} w^{k}_{3j} + \\cdots + a^{k-1}_{n} w^{k}_{nj} + b^k_j$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La red neuronal toma el valor de entrada en la primera capa y, a trav√©s de los pesos y sesgos, se forman las neuronas siguientes en la capa siguiente:\n",
    "\n",
    "$a^1_1$ es la primera neurona en la primera capa, que corresponde al resultado de los datos de entrada dados.\n",
    "\n",
    "$a^2_1 = \\sigma(a^1_1 w^2_{11} + a^1_2 w^2_{21} + a^1_3 w^2_{31} + \\cdots + a^1_n w^2_{n1} + b^2_1)$\n",
    "\n",
    "$a^2_2 = \\sigma(a^1_1 w^2_{12} + a^1_2 w^2_{22} + a^1_3 w^2_{32} + \\cdots + a^1_n w^2_{n2} + b^2_2)$\n",
    "\n",
    "$...$\n",
    "\n",
    "$a^2_m = \\sigma(a^1_1 w^2_{1m} + a^1_2 w^2_{2m} + a^1_3 w^2_{3m} + \\cdots + a^1_n w^2_{nm} + b^2_m)$\n",
    "\n",
    "A partir de esto, se forma la siguiente capa de neuronas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importancia de las Funciones de Activaci√≥n en Redes Neuronales\n",
    "\n",
    "La funci√≥n de activaci√≥n cumple un papel crucial en una red neuronal al introducir no linealidad en la salida de cada neurona. Sin esta no linealidad, una red neuronal se reducir√≠a esencialmente a un modelo lineal, limitando severamente su capacidad expresiva y de aprendizaje. A continuaci√≥n, las razones por las que las funciones de activaci√≥n son importantes, junto con algunos ejemplos:\n",
    "\n",
    "### 1. **Introducci√≥n de No Linealidad:**\n",
    "Las funciones de activaci√≥n permiten a las redes neuronales modelar relaciones complejas y no lineales en los datos. Muchos problemas del mundo real involucran patrones no lineales, y las funciones de activaci√≥n habilitan a la red para capturar estos patrones de forma efectiva.\n",
    "\n",
    "### 2. **Habilitar Representaciones Complejas:**\n",
    "Al introducir no linealidad, las funciones de activaci√≥n permiten que las redes neuronales aprendan representaciones complejas de los datos de entrada. Esto es esencial para tareas como el reconocimiento de im√°genes, procesamiento de lenguaje natural y predicci√≥n de series temporales, donde las relaciones entre entrada y salida son altamente no lineales.\n",
    "\n",
    "### 3. **Normalizaci√≥n y Estabilizaci√≥n:**\n",
    "Las funciones de activaci√≥n ayudan a normalizar la salida de las neuronas, evitando que crezcan demasiado o se reduzcan excesivamente durante el proceso de entrenamiento. Esto contribuye a estabilizar el aprendizaje y a prevenir problemas como el desvanecimiento o explosi√≥n del gradiente.\n",
    "\n",
    "---\n",
    "\n",
    "### Ejemplos de Funciones de Activaci√≥n Comunes:\n",
    "\n",
    "- **Funci√≥n Sigmoide:**  \n",
    "  La funci√≥n sigmoide comprime los valores de entrada al rango [0, 1]. Se usa frecuentemente en problemas de clasificaci√≥n binaria donde la salida debe interpretarse como una probabilidad. Sin embargo, sufre del problema de gradientes que se desvanecen, lo que puede ralentizar el entrenamiento, especialmente en redes profundas.\n",
    "\n",
    "- **Funci√≥n Tangente Hiperb√≥lica (tanh):**  \n",
    "  Similar a la sigmoide, la funci√≥n tanh comprime los valores al rango [-1, 1]. Es com√∫n en redes neuronales, especialmente cuando los datos de entrada est√°n centrados en cero. Al igual que la sigmoide, puede sufrir el problema de gradientes que se desvanecen.\n",
    "\n",
    "- **Unidad Lineal Rectificada (ReLU):**  \n",
    "  La funci√≥n ReLU devuelve 0 para entradas negativas y el valor de entrada para entradas positivas. Se ha convertido en la elecci√≥n predeterminada para muchas arquitecturas de redes neuronales debido a su simplicidad y eficacia pr√°ctica. ReLU ayuda a mitigar el problema del gradiente que se desvanece y acelera el proceso de entrenamiento, especialmente en redes profundas.\n",
    "\n",
    "- **Leaky ReLU:**  \n",
    "  Leaky ReLU es una variante de ReLU que permite un peque√±o gradiente no nulo para entradas negativas. Esto ayuda a resolver el problema de \"ReLU muerta\", donde neuronas pueden volverse inactivas durante el entrenamiento y dejar de aprender.\n",
    "\n",
    "- **Funci√≥n Softmax:**  \n",
    "  La funci√≥n softmax se usa com√∫nmente en la capa de salida de redes neuronales para tareas de clasificaci√≥n multiclase. Convierte los valores de salida en una distribuci√≥n de probabilidad sobre m√∫ltiples clases, haciendo que sea adecuada para predecir probabilidades de clase.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algoritmo de Retropropagaci√≥n (Backpropagation)\n",
    "\n",
    "Desglosemos el algoritmo de retropropagaci√≥n en pasos manejables:\n",
    "\n",
    "1. **Paso hacia adelante (Forward Pass):**  \n",
    "   Comenzamos realizando un pase hacia adelante a trav√©s de la red neuronal. Esto implica alimentar los datos de entrada en la red y calcular la salida predicha. Mientras propagamos los datos capa por capa, almacenamos los resultados intermedios, como las activaciones de cada neurona.\n",
    "\n",
    "2. **C√°lculo de la funci√≥n de p√©rdida:**  \n",
    "   Una vez obtenida la salida predicha, la comparamos con la salida real (verdad terreno) para calcular la p√©rdida. La funci√≥n de p√©rdida representa qu√© tan lejos est√°n nuestras predicciones de los valores reales. Las funciones de p√©rdida comunes incluyen el error cuadr√°tico medio (MSE) para tareas de regresi√≥n y la entrop√≠a cruzada para tareas de clasificaci√≥n.\n",
    "\n",
    "3. **Paso hacia atr√°s (Backward Pass):**  \n",
    "   Aqu√≠ est√° el n√∫cleo del algoritmo de retropropagaci√≥n: el pase hacia atr√°s. En este paso, calculamos el gradiente de la funci√≥n de p√©rdida respecto a cada par√°metro de la red. Partimos desde la capa de salida y avanzamos hacia atr√°s por la red, aplicando la regla de la cadena del c√°lculo diferencial para obtener los gradientes capa por capa.\n",
    "\n",
    "4. **Actualizaci√≥n de pesos:**  \n",
    "   Finalmente, usamos los gradientes calculados en el paso hacia atr√°s para actualizar los pesos de la red. Lo hacemos restando una fracci√≥n del gradiente a cada peso, escalada por un par√°metro llamado tasa de aprendizaje (learning rate). Este proceso ajusta los par√°metros en la direcci√≥n que minimiza la p√©rdida, mejorando gradualmente el desempe√±o de la red.\n",
    "\n",
    "5. **Repetici√≥n:**  \n",
    "   Repetimos este proceso por un n√∫mero fijo de iteraciones (√©pocas) o hasta que la red converja a un nivel satisfactorio de desempe√±o. Despu√©s de cada iteraci√≥n, la red mejora un poco en sus predicciones, gracias a la retroalimentaci√≥n proporcionada por el algoritmo de retropropagaci√≥n.\n",
    "\n",
    "Y esa es la esencia del algoritmo de retropropagaci√≥n. Es una t√©cnica poderosa que sustenta el entrenamiento de redes neuronales, permiti√©ndoles aprender patrones y relaciones complejas en los datos. Comprender c√≥mo funciona la retropropagaci√≥n nos da una visi√≥n profunda de c√≥mo las redes neuronales aprenden y mejoran su desempe√±o con el tiempo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funci√≥n de Costo\n",
    "\n",
    "La funci√≥n de costo, tambi√©n conocida como funci√≥n de p√©rdida u funci√≥n objetivo, es un componente crucial en el entrenamiento de una red neuronal. Su rol principal es cuantificar qu√© tan bien las predicciones del modelo coinciden con los valores reales. El objetivo durante el entrenamiento es minimizar esta funci√≥n de costo, reduciendo efectivamente la disparidad entre las predicciones y la verdad terreno.\n",
    "\n",
    "La elecci√≥n de la funci√≥n de costo depende de la naturaleza del problema a resolver. Aqu√≠ algunos ejemplos comunes:\n",
    "\n",
    "- **Error Cuadr√°tico Medio (MSE):**  \n",
    "  Es una funci√≥n de costo ampliamente usada para problemas de regresi√≥n. Calcula el promedio de las diferencias al cuadrado entre los valores predichos y los valores reales. MSE penaliza los errores grandes de forma m√°s severa que los peque√±os, por lo que es adecuada para tareas donde se requieren predicciones num√©ricas precisas.\n",
    "\n",
    "  $$MSE = (1/n) ‚àë·µ¢= (y·µ¢ ‚àí ≈∑·µ¢)¬≤$$\n",
    "\n",
    "- **Entrop√≠a Cruzada (Cross-Entropy Loss):**  \n",
    "  Com√∫nmente usada para tareas de clasificaci√≥n, la entrop√≠a cruzada mide la diferencia entre la distribuci√≥n de probabilidad predicha y la distribuci√≥n real de las etiquetas de clase. Tiende a penalizar los errores de clasificaci√≥n m√°s fuertemente que las clasificaciones correctas, siendo adecuada para problemas de clasificaci√≥n multiclase.\n",
    "\n",
    "  $$Cross-Entropy Loss = ‚àí(1/n) ‚àë·µ¢= y·µ¢ log(≈∑·µ¢)$$\n",
    "\n",
    "- **Entrop√≠a Cruzada Binaria (Binary Cross-Entropy Loss):**  \n",
    "  Caso espec√≠fico de la entrop√≠a cruzada usada en clasificaci√≥n binaria, donde solo hay dos posibles resultados.\n",
    "\n",
    "  $$Binary Cross-Entropy Loss = ‚àí(1/n) ‚àë·µ¢= [y·µ¢ log(≈∑·µ¢) + (1 ‚àí y·µ¢) log(1 ‚àí ≈∑·µ¢)]$$\n",
    "\n",
    "- **P√©rdida Hinge (Hinge Loss):**  \n",
    "  Usada com√∫nmente para m√°quinas de vectores de soporte (SVM) y tareas de clasificaci√≥n binaria. Penaliza las malas clasificaciones, pero a diferencia de la entrop√≠a cruzada, no produce probabilidades directamente.\n",
    "\n",
    "  $$Hinge Loss = max(0, 1 ‚àí y * ≈∑·µ¢)$$\n",
    "\n",
    "- **P√©rdida Huber (Huber Loss):**  \n",
    "  Funci√≥n de p√©rdida robusta usada en tareas de regresi√≥n, menos sensible a valores at√≠picos que el MSE. Combina el error cuadr√°tico para valores peque√±os y el error absoluto para valores grandes.\n",
    "\n",
    "---\n",
    "\n",
    "Al minimizar la funci√≥n de costo elegida usando algoritmos de optimizaci√≥n como el gradiente descendente o sus variantes, actualizamos iterativamente los par√°metros de la red neuronal para mejorar su desempe√±o en la tarea. La selecci√≥n de una funci√≥n de costo adecuada es crucial, ya que influye directamente en el comportamiento y eficacia del proceso de aprendizaje.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Variantes del Descenso de Gradiente\n",
    "\n",
    "Existen varias variantes del descenso de gradiente para mejorar su desempe√±o, especialmente en conjuntos de datos grandes y redes neuronales complejas:\n",
    "\n",
    "## Descenso de Gradiente por Lote (Batch Gradient Descent):\n",
    "\n",
    "- Utiliza todo el conjunto de datos para calcular el gradiente de la funci√≥n de p√©rdida.\n",
    "- **Ventajas:** Proporciona una estimaci√≥n estable y precisa del gradiente.\n",
    "- **Desventajas:** Puede ser muy lento y costoso computacionalmente para conjuntos de datos grandes.\n",
    "\n",
    "## Descenso de Gradiente Estoc√°stico (Stochastic Gradient Descent, SGD):\n",
    "\n",
    "- Actualiza los par√°metros para cada ejemplo de entrenamiento individualmente.\n",
    "- **Ventajas:** M√°s r√°pido y puede escapar m√°s f√°cilmente de m√≠nimos locales.\n",
    "- **Desventajas:** Las estimaciones del gradiente pueden tener alta varianza, lo que causa fluctuaciones en la funci√≥n de p√©rdida.\n",
    "\n",
    "## Descenso de Gradiente por Mini-Lotes (Mini-Batch Gradient Descent):\n",
    "\n",
    "- Utiliza un peque√±o subconjunto aleatorio del conjunto de datos (mini-lote) para calcular el gradiente.\n",
    "- **Ventajas:** Equilibra los beneficios del descenso por lote y estoc√°stico. Es computacionalmente eficiente y proporciona actualizaciones m√°s estables que el SGD.\n",
    "- **Desventajas:** Requiere ajustar el tama√±o del mini-lote.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explicacion matematica de como funciona el backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1-Fourth Layer\n",
    "$$CostFunction=C=(a^4_1-C_1)^2+(a^4_2-C_2)^2+(a^4_3-C_3)^2$$\n",
    "# Example\n",
    "$$\\frac{\\partial C}{\\partial w^4_{11}}=\\frac{\\partial C}{\\partial a^4_{1}}*\\frac{\\partial a^4_1}{\\partial z^4_{1}}*\\frac{\\partial z^4_1}{\\partial w^4_{11}}$$\n",
    "$$\\frac{\\partial C}{\\partial w^4_{23}}=\\frac{\\partial C}{\\partial a^4_{3}}*\\frac{\\partial a^4_3}{\\partial z^4_{3}}*\\frac{\\partial z^4_3}{\\partial w^4_{23}}$$\n",
    "$$\\frac{\\partial C}{\\partial b^4_{1}}=\\frac{\\partial C}{\\partial a^4_{1}}*\\frac{\\partial a^4_1}{\\partial z^4_{1}}*\\frac{\\partial z^4_1}{\\partial b^4_{1}}$$\n",
    "$$\\frac{\\partial C}{\\partial b^4_{3}}=\\frac{\\partial C}{\\partial a^4_{3}}*\\frac{\\partial a^4_3}{\\partial z^4_{3}}*\\frac{\\partial z^4_3}{\\partial b^4_{3}}$$\n",
    "\n",
    "# Generalization\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial w^k_{ij}}=\\frac{\\partial C}{\\partial a^k_{j}}*\\frac{\\partial a^k_j}{\\partial z^k_{j}}*\\frac{\\partial z^k_j}{\\partial w^k_{ij}}$$\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial b^k_{j}}=\\frac{\\partial C}{\\partial a^k_{j}}*\\frac{\\partial a^k_j}{\\partial z^k_{j}}*\\frac{\\partial z^k_j}{\\partial b^k_{j}}$$\n",
    "\n",
    "# Derivatives solved\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial w^k_{ij}}=2(a^k_j-C_j)*\\sigma '(z^k_j)a^{k-1}_i$$\n",
    "$$\\frac{\\partial C}{\\partial b^k_{j}}=2(a^k_j-C_j)*\\sigma '(z^k_j)*1$$\n",
    "\n",
    "# Third layer\n",
    "\n",
    "# Example\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial w^3_{12}}=\\frac{\\partial C}{\\partial a^4_{1}}*\\frac{\\partial  a^4_{1}}{\\partial z^4_{1}}*\\frac{\\partial  z^4_{1}}{\\partial a^3_{2}}*\\frac{\\partial  a^3_{2}}{\\partial z^3_{2}}*\\frac{\\partial  z^3_{2}}{\\partial w^3_{12}}+\\frac{\\partial C}{\\partial a^4_{2}}*\\frac{\\partial  a^4_{2}}{\\partial z^4_{2}}*\\frac{\\partial  z^4_{2}}{\\partial a^3_{2}}*\\frac{\\partial  a^3_{2}}{\\partial z^3_{2}}*\\frac{\\partial  z^3_{2}}{\\partial w^3_{12}} +\\frac{\\partial C}{\\partial a^4_{3}}*\\frac{\\partial  a^4_{3}}{\\partial z^4_{3}}*\\frac{\\partial  z^4_{3}}{\\partial a^3_{2}}*\\frac{\\partial  a^3_{2}}{\\partial z^3_{2}}*\\frac{\\partial  z^3_{2}}{\\partial w^3_{12}}$$\n",
    "\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial b^3_{2}}=\\frac{\\partial C}{\\partial a^4_{1}}*\\frac{\\partial  a^4_{1}}{\\partial z^4_{1}}*\\frac{\\partial  z^4_{1}}{\\partial a^3_{2}}*\\frac{\\partial  a^3_{2}}{\\partial z^3_{2}}*\\frac{\\partial  z^3_{2}}{\\partial b^3_{2}}+\\frac{\\partial C}{\\partial a^4_{2}}*\\frac{\\partial  a^4_{2}}{\\partial z^4_{2}}*\\frac{\\partial  z^4_{2}}{\\partial a^3_{2}}*\\frac{\\partial  a^3_{2}}{\\partial z^3_{2}}*\\frac{\\partial  z^3_{2}}{\\partial b^3_{2}} +\\frac{\\partial C}{\\partial a^4_{3}}*\\frac{\\partial  a^4_{3}}{\\partial z^4_{3}}*\\frac{\\partial  z^4_{3}}{\\partial a^3_{2}}*\\frac{\\partial  a^3_{2}}{\\partial z^3_{2}}*\\frac{\\partial  z^3_{2}}{\\partial b^3_{2}}$$\n",
    "\n",
    "# Generalization\n",
    "\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial w^{k-1}_{ij}}=\\frac{\\partial C}{\\partial a^k_{1}}*\\frac{\\partial  a^k_{1}}{\\partial z^k_{1}}*\\frac{\\partial  z^k_{1}}{\\partial a^{k-1}_{j}}*\\frac{\\partial  a^{k-1}_{j}}{\\partial z^{k-1}_{j}}*\\frac{\\partial  z^{k-1}_{j}}{\\partial w^{k-1}_{ij}}+\\frac{\\partial C}{\\partial a^k_{2}}*\\frac{\\partial  a^k_{2}}{\\partial z^k_{2}}*\\frac{\\partial  z^k_{2}}{\\partial a^{k-1}_{j}}*\\frac{\\partial  a^{k-1}_{j}}{\\partial z^{k-1}_{j}}*\\frac{\\partial  z^{k-1}_{j}}{\\partial w^{k-1}_{ij}} +\\frac{\\partial C}{\\partial a^k_{3}}*\\frac{\\partial  a^k_{3}}{\\partial z^k_{3}}*\\frac{\\partial  z^k_{3}}{\\partial a^{k-1}_{j}}*\\frac{\\partial  a^{k-1}_{j}}{\\partial z^{k-1}_{j}}*\\frac{\\partial  z^{k-1}_{j}}{\\partial w^{k-1}_{ij}}$$\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial b^{k-1}_{j}}=\\frac{\\partial C}{\\partial a^k_{1}}*\\frac{\\partial  a^k_{1}}{\\partial z^k_{1}}*\\frac{\\partial  z^k_{1}}{\\partial a^{k-1}_{j}}*\\frac{\\partial  a^{k-1}_{j}}{\\partial z^{k-1}_{j}}*\\frac{\\partial  z^{k-1}_{j}}{\\partial b^{k-1}_{j}}+\\frac{\\partial C}{\\partial a^k_{2}}*\\frac{\\partial  a^k_{2}}{\\partial z^k_{2}}*\\frac{\\partial  z^k_{2}}{\\partial a^{k-1}_{j}}*\\frac{\\partial  a^{k-1}_{j}}{\\partial z^{k-1}_{j}}*\\frac{\\partial  z^{k-1}_{j}}{\\partial b^{k-1}_{j}} +\\frac{\\partial C}{\\partial a^k_{3}}*\\frac{\\partial  a^k_{3}}{\\partial z^k_{3}}*\\frac{\\partial  z^k_{3}}{\\partial a^{k-1}_{j}}*\\frac{\\partial  a^{k-1}_{j}}{\\partial z^{k-1}_{j}}*\\frac{\\partial  z^{k-1}_{j}}{\\partial b^{k-1}_{j}}$$\n",
    "\n",
    "# Formula \n",
    "\n",
    "$$\\frac{\\partial C}{\\partial w^{k-1}_{ij}}=\\sum^3_{m=1}\\frac{\\partial C}{\\partial a^k_{m}}*\\frac{\\partial  a^k_{m}}{\\partial z^k_{m}}*\\frac{\\partial  z^k_{m}}{\\partial a^{k-1}_{j}}*\\frac{\\partial  a^{k-1}_{j}}{\\partial z^{k-1}_{j}}*\\frac{\\partial  z^{k-1}_{j}}{\\partial w^{k-1}_{ij}}$$\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial b^{k-1}_{j}}=\\sum^3_{m=1}\\frac{\\partial C}{\\partial a^k_{m}}*\\frac{\\partial  a^k_{m}}{\\partial z^k_{m}}*\\frac{\\partial  z^k_{m}}{\\partial a^{k-1}_{j}}*\\frac{\\partial  a^{k-1}_{j}}{\\partial z^{k-1}_{j}}*\\frac{\\partial  z^{k-1}_{j}}{\\partial b^{k-1}_{j}}$$\n",
    "\n",
    "# Derivatives solved\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial w^{k-1}_{ij}}=\\sum^3_{m=1}2(a^k_m-C_m)*\\sigma '(z^k_m)*w^k_{jm}*\\sigma ' (z^{k-1}_j)*a^{k-2}_i$$\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial b^{k-1}_{j}}=\\sum^3_{m=1}2(a^k_m-C_m)*\\sigma '(z^k_m)*w^k_{jm}*\\sigma ' (z^{k-1}_j)*1$$\n",
    "\n",
    "# Second Layer\n",
    "\n",
    "# Example\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial w^{2}_{31}}=\\frac{\\partial C}{\\partial a^4_1}*\\frac{\\partial a^4_1}{\\partial z^4_1}*(\\frac{\\partial z^4_1}{\\partial a^3_1}*\\frac{\\partial a^3_1}{\\partial z^3_1}*\\frac{\\partial z^3_1}{\\partial a^2_1}*\\frac{\\partial a^2_1}{\\partial z^2_1}*\\frac{\\partial z^2_1}{\\partial w^2_{31}}+\\frac{\\partial z^4_1}{\\partial a^3_2}*\\frac{\\partial a^3_2}{\\partial z^3_2}*\\frac{\\partial z^3_2}{\\partial a^2_1}*\\frac{\\partial a^2_1}{\\partial z^2_1}*\\frac{\\partial z^2_1}{\\partial w^2_{31}}) +\\frac{\\partial C}{\\partial a^4_2}*\\frac{\\partial a^4_2}{\\partial z^4_2}*(\\frac{\\partial z^4_2}{\\partial a^3_1}*\\frac{\\partial a^3_1}{\\partial z^3_1}*\\frac{\\partial z^3_1}{\\partial a^2_1}*\\frac{\\partial a^2_1}{\\partial z^2_1}*\\frac{\\partial z^2_1}{\\partial w^2_{31}}+\\frac{\\partial z^4_2}{\\partial a^3_2}*\\frac{\\partial a^3_2}{\\partial z^3_2}*\\frac{\\partial z^3_2}{\\partial a^2_1}*\\frac{\\partial a^2_1}{\\partial z^2_1}*\\frac{\\partial z^2_1}{\\partial w^2_{31}})+\\frac{\\partial C}{\\partial a^4_3}*\\frac{\\partial a^4_3}{\\partial z^4_3}*(\\frac{\\partial z^4_3}{\\partial a^3_1}*\\frac{\\partial a^3_1}{\\partial z^3_1}*\\frac{\\partial z^3_1}{\\partial a^2_1}*\\frac{\\partial a^2_1}{\\partial z^2_1}*\\frac{\\partial z^2_1}{\\partial w^2_{31}}+\\frac{\\partial z^4_3}{\\partial a^3_2}*\\frac{\\partial a^3_2}{\\partial z^3_2}*\\frac{\\partial z^3_2}{\\partial a^2_1}*\\frac{\\partial a^2_1}{\\partial z^2_1}*\\frac{\\partial z^2_1}{\\partial w^2_{31}})$$\n",
    "\n",
    "\n",
    "# Formula \n",
    "\n",
    "$$\\frac{\\partial C}{\\partial w^{k-2}_{ij}}=\\sum^3_{m=1}\\frac{\\partial C}{\\partial a^k_m}*\\frac{\\partial a^k_m}{\\partial z^k_m}\\sum^2_{p=1}\\frac{\\partial z^k_m}{\\partial a^{k-1}_p}*\\frac{\\partial a^{k-1}_p}{\\partial z^{k-1}_p}*\\frac{\\partial z^{k-1}_p }{\\partial a^{k-2}_j}*\\frac{\\partial a^{k-2}_j }{\\partial z^{k-2}_j}*\\frac{\\partial z^{k-2}_j }{\\partial w^{k-2}_{ij}}$$\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial b^{k-2}_{j}}=\\sum^3_{m=1}\\frac{\\partial C}{\\partial a^k_m}*\\frac{\\partial a^k_m}{\\partial z^k_m}\\sum^2_{p=1}\\frac{\\partial z^k_m}{\\partial a^{k-1}_p}*\\frac{\\partial a^{k-1}_p}{\\partial z^{k-1}_p}*\\frac{\\partial z^{k-1}_p }{\\partial a^{k-2}_j}*\\frac{\\partial a^{k-2}_j }{\\partial z^{k-2}_j}*\\frac{\\partial z^{k-2}_j }{\\partial b^{k-2}_{j}}$$\n",
    "\n",
    "# Derivatives solved\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial w^{k-2}_{ij}}=\\sum^3_{m=1}2(a^k_m-C_m)*\\sigma '(z^k_m)\\sum^2_{p=1}w^k_{pm}*\\sigma '(z^{k-1}_p)*w^{k-1}_{jp}*\\sigma '(z^{k-2}_j)*a^{k-3}_i$$\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial b^{k-2}_{j}}=\\sum^3_{m=1}2(a^k_m-C_m)*\\sigma '(z^k_m)\\sum^2_{p=1}w^k_{pm}*\\sigma '(z^{k-1}_p)*w^{k-1}_{jp}*\\sigma '(z^{k-2}_j)*1$$\n",
    "\n",
    "# The general formula for any amount of hidden layer and x amount of neuron for layer\n",
    "\n",
    "First a change in terminology $m=i_k$, $p=i_{k-1}$...\n",
    "\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial w^{k-n}_{ij}} = \\frac{1}{N_k} \\sum_{i_k=1}^{N_k} 2(a^k_{i_k} - C_{i_k}) \\sigma'(z^k_{i_k}) \n",
    "\\sum_{i_{k-1}=1}^{N_{k-1}} w^k_{i_{k-1}i_k} \\sigma'(z^{k-1}_{i_{k-1}}) \n",
    "\\sum_{i_{k-2}=1}^{N_{k-2}} w^{k-1}_{i_{k-2}i_{k-1}} \\sigma'(z^{k-2}_{i_{k-2}}) \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sum_{i_{k-3}=1}^{N_{k-3}} w^{k-2}_{i_{k-3}i_{k-2}} \\sigma'(z^{k-3}_{i_{k-3}}) \\cdots \n",
    "\\sum_{i_{k-n+1}=1}^{N_{k-n+1}} w^{k-n+2}_{i_{k-n+1}i_{k-n+2}} \\sigma'(z^{k-n+1}_{i_{k-n+1}}) \n",
    "w^{k-n+1}_{ji_{k-n+1}} \\sigma'(z^{k-n}_j) a^{k-n-1}_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial b^{k-n}_{j}} = \\frac{1}{N_k} \\sum_{i_k=1}^{N_k} 2(a^k_{i_k} - C_{i_k}) \\sigma'(z^k_{i_k}) \\sum_{i_{k-1}=1}^{N_{k-1}} w^k_{i_{k-1}i_k} \\sigma'(z^{k-1}_{i_{k-1}}) \\sum_{i_{k-2}=1}^{N_{k-2}} w^{k-1}_{i_{k-2}i_{k-1}} \\sigma'(z^{k-2}_{i_{k-2}}) \\sum_{i_{k-3}=1}^{N_{k-3}} w^{k-2}_{i_{k-3}i_{k-2}} \\sigma'(z^{k-3}_{i_{k-3}}) \\cdots \\sum_{i_{k-n+1}=1}^{N_{k-n+1}} w^{k-n+2}_{i_{k-n+1}i_{k-n+2}} \\sigma'(z^{k-n+1}_{i_{k-n+1}}) w^{k-n+1}_{ji_{k-n+1}} \\sigma'(z^{k-n}_j) \\cdot 1\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convirtiendo la suma a multiplicacion de matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1- First order\n",
    "\n",
    "### Derivatives solved\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial w^k_{i_{k-1}i_{k}}}=2(a^k_{i_{k}}-C_{i_{k}})*\\sigma '(z^k_{i_{k}})a^{k-1}_{i_{k-1}}$$\n",
    "\n",
    "\n",
    "Assuming vectors as rows and using the variable $A^1_{i_k}$:\n",
    "\n",
    "$$A^1_{i_k}=2(a^k_{i_{k}}-C_{i_{k}})*\\sigma '(z^k_{i_{k}})$$\n",
    "\n",
    "$$\n",
    "\\left( \\frac{\\partial C}{\\partial w^k_{i_{k-1},i_{k}}} \\right)_{k-1,k} = \n",
    "\\left( \n",
    "\\left( \n",
    "\\left( a^{k-1}_{i_{k-1}} \\right)_{k-1,1}^T \n",
    "\\cdot \n",
    "\\left( A^1_{i_k} \\right)_{1,k} \n",
    "\\right) \n",
    "\\right)_{k-1,k}\n",
    "$$\n",
    "\n",
    "### 2- Second order\n",
    "\n",
    "### Derivatives solved\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial w^{k-1}_{i_{k-2}i_{k-1}}}=\\sum^{N_k}_{i_{k}=1}2(a^k_{i_{k}}-C_{i_{k}})*\\sigma '(z^k_{i_{k}})*w^k_{i_{k-1}i_{k}}*\\sigma ' (z^{k-1}_{i_{k-1}})*a^{k-2}_{i_{k-2}}$$\n",
    "\n",
    "Using the variable $A^1_{i_k}$ y $A^2_{i_{k-1}i_{k}}$\n",
    "\n",
    "\n",
    "$$A^1_{i_k}=2(a^k_{i_{k}}-C_{i_{k}})*\\sigma '(z^k_{i_{k-1}})$$\n",
    "\n",
    "\n",
    "\n",
    "$$A^2_{i_{k-1}i_{k}}= w^k_{i_{k-1}i_{k}}*\\sigma ' (z^{k-1}_{i_{k-1}}) $$\n",
    "\n",
    "We have the following formula:\n",
    "\n",
    "\n",
    "$$\n",
    "\\left( \\frac{\\partial C}{\\partial w^{k-1}_{i_{k-2},i_{k-1}}}\\right)_{k-2,k-1} = \n",
    "\\left( \n",
    "\\left( \n",
    "\\left( \n",
    "A^2_{i_{k-1},i_{k}} \n",
    "\\cdot (A^1_{i_{k}})^T_{i_{k},1} \n",
    "\\right)_{i_{k-1},1} \n",
    "\\cdot (a^{k-2}_{i_{k-2}})_{1,i_{k-2}} \n",
    "\\right) \n",
    "\\right)^T_{k-2,k-1}\n",
    "$$\n",
    "\n",
    "### 3- Third order\n",
    "\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial w^{k-2}_{i_{k-3},i_{k-2}}} = \n",
    "\\sum_{i_{k}=1}^{N_k} 2(a^k_{i_k} - C_{i_k}) \\cdot \\sigma'(z^k_{i_k}) \n",
    "\\sum_{i_{k-1}=1}^{N_{k-1}} w^k_{i_{k-1},i_k} \\cdot \\sigma'(z^{k-1}_{i_{k-1}}) \n",
    "\\cdot w^{k-1}_{i_{k-2},i_{k-1}} \\cdot \\sigma'(z^{k-2}_{i_{k-2}}) \n",
    "\\cdot a^{k-3}_{i_{k-3}}\n",
    "$$\n",
    "\n",
    "Using the variables $A^1_{i_k}$ , $A^2_{i_{k-1}i_{k}}$ y $A^3_{i_{k-2}i_{k-1}}$:\n",
    "\n",
    "$$A^1_{i_k}=2(a^k_{i_{k}}-C_{i_{k}})*\\sigma '(z^k_{i_{k-1}})$$\n",
    "\n",
    "$$A^2_{i_{k-1}i_{k}}= w^k_{i_{k-1}i_{k}}*\\sigma ' (z^{k-1}_{i_{k-1}}) $$\n",
    "\n",
    "$$A^3_{i_{k-2}i_{k-1}}=w^{k-1}_{i_{k-2},i_{k-1}} \\cdot \\sigma'(z^{k-2}_{i_{k-2}}) $$\n",
    "\n",
    "We have the following formula:\n",
    "\n",
    "$$\\left( \\left[ \\left( A^3_{i_{k-2}i_{k-1}}\\right)_{k-2,k-1}\\cdot\\left[ \\left( A^2_{i_{k-1}i_{k}}\\right)_{k-1,k}\\cdot \\left(A^1_{i_k}\\right)^T_{k,1}\\right]\\right]\\cdot\\left(a^{k-3}_{i_{k-3}} \\right)_{1,k-3} \\right)^T$$\n",
    "\n",
    "### 4- Fourt order\n",
    "\n",
    "$$\\left(\\frac{\\partial C}{\\partial w^{k-3}_{i_{k-4},i_{k-3}}}\\right)_{k-4,k-3}=\\left[ \\left( \\left(A^4_{i_{k-3}i_{k-2}} \\right)_{k-3,k-2} \\cdot \\left[ \\left( A^3_{i_{k-2}i_{k-1}}\\right)_{k-2,k-1} \\cdot \\left(\\left(A^2_{i_{k-1}i_{k}}\\right)_{k-1,k} \\cdot \\left(A^1_{i_k}\\right)^T_{k,1}  \\right)_{k-1,1}\\right]\\right)\\cdot \\left(a^{k-4}_{i_{k-4}} \\right)_{1,k-4}\\right]^T$$\n",
    "\n",
    ".\n",
    ".\n",
    ".\n",
    "\n",
    "### N- N order\n",
    "\n",
    "$$\\left(\\frac{\\partial C}{\\partial w^{k-N}_{i_{k-N-1},i_{k-N}}}\\right)_{k-N-1,k-N}=\\left( \\left[\\left(A^{N+1}_{i_{k-N}i_{k-N-1}}\\right) \\cdots \\left( \\left(A^4_{i_{k-3}i_{k-2}} \\right)_{k-3,k-2} \\cdot \\left[ \\left( A^3_{i_{k-2}i_{k-1}}\\right)_{k-2,k-1} \\cdot \\left(\\left(A^2_{i_{k-1}i_{k}}\\right)_{k-1,k} \\cdot \\left(A^1_{i_k}\\right)^T_{k,1}  \\right)_{k-1,1}\\right]\\right)\\right]\\cdot \\left(a^{k-4}_{i_{k-4}} \\right)_{1,k-4}\\right)^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. [Red Neuronal del premio nobel](#applications-of-cnns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"hop.png\" width=\"4000\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las **Hofield Neural Networks** son un tipo especializado de redes neuronales dise√±adas para capturar relaciones complejas en datos con estructuras espaciales o temporales, aprovechando t√©cnicas avanzadas de aprendizaje profundo. Estas redes combinan conceptos de modelado probabil√≠stico y redes neuronales para abordar problemas donde la correlaci√≥n entre variables juega un papel fundamental.\n",
    "\n",
    "Este enfoque es especialmente √∫til en √°reas como procesamiento de se√±ales, an√°lisis de im√°genes y modelado de series temporales, donde las dependencias no lineales y las interacciones entre componentes deben ser aprendidas eficazmente. Las Hofield Neural Networks integran mecanismos que permiten representar din√°micas complejas y pueden mejorar significativamente el desempe√±o en tareas de predicci√≥n y clasificaci√≥n.\n",
    "\n",
    "El estudio y aplicaci√≥n de estas redes representa una frontera en la investigaci√≥n en inteligencia artificial, combinando teor√≠a matem√°tica avanzada con implementaciones pr√°cticas en diversas disciplinas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# - 4.1 [Hopfield Neural Network](#convolution)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teor√≠a de las Redes de Hopfield\n",
    "\n",
    "Las **Redes de Hopfield** son un tipo de red neuronal recurrente desarrollada por John Hopfield en 1982. Est√°n inspiradas en los sistemas f√≠sicos que buscan estados de energ√≠a m√≠nima, como los sistemas magn√©ticos de Ising, y se utilizan principalmente como modelos de memoria asociativa. Estas redes son capaces de almacenar patrones y recuperarlos incluso cuando las entradas contienen ruido o est√°n incompletas.\n",
    "\n",
    "## Caracter√≠sticas Principales:\n",
    "\n",
    "- Las neuronas est√°n completamente conectadas entre s√≠, excepto consigo mismas ($w_{ii} = 0$).\n",
    "- Las conexiones son **sim√©tricas**: $w_{ij} = w_{ji}$.\n",
    "- El estado de cada neurona es binario: $s_i \\in \\{-1, 1\\}$.\n",
    "- Evolucionan buscando un m√≠nimo de una **funci√≥n de energ√≠a**.\n",
    "\n",
    "---\n",
    "\n",
    "## Funci√≥n de Energ√≠a de Hopfield\n",
    "\n",
    "La red evoluciona hacia un estado que minimiza la **energ√≠a del sistema**. La funci√≥n de energ√≠a asociada es:\n",
    "\n",
    "$$\n",
    "E = -\\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n w_{ij} s_i s_j \n",
    "$$\n",
    "\n",
    "Donde:\n",
    "- $s_i$ es el estado de la neurona $i$.\n",
    "- $w_{ij}$ es el peso de conexi√≥n entre las neuronas $i$ y $j$.\n",
    "-\n",
    "\n",
    "La red evoluciona modificando los estados $s_i$ para minimizar esta funci√≥n de energ√≠a.\n",
    "\n",
    "---\n",
    "\n",
    "## Din√°mica de Actualizaci√≥n\n",
    "\n",
    "La din√°mica t√≠pica de actualizaci√≥n de las neuronas es asincr√≥nica, es decir, se actualiza una neurona a la vez de forma secuencial o aleatoria. La regla de actualizaci√≥n para la neurona $i$ es:\n",
    "\n",
    "$$\n",
    "s_i \\leftarrow \\text{sign}\\left( \\sum_{j=1}^n w_{ij} s_j \\right)\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "- $\\text{sign}(\\cdot)$ es la funci√≥n signo que devuelve $1$ si la entrada es positiva y $-1$ si es negativa.\n",
    "\n",
    "---\n",
    "\n",
    "## Almacenamiento de Patrones (Hebb's Rule)\n",
    "\n",
    "Los pesos de la red pueden determinarse utilizando la regla de Hebb para almacenar un conjunto de $p$ patrones $\\{\\mathbf{s}^{(1)}, \\mathbf{s}^{(2)}, \\dots, \\mathbf{s}^{(p)}\\}$:\n",
    "\n",
    "$$\n",
    "w_{ij} = \\frac{1}{n} \\sum_{\\mu=1}^p s_i^{(\\mu)} s_j^{(\\mu)}, \\quad \\text{con} \\quad w_{ii} = 0\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "- $s_i^{(\\mu)}$ es el valor de la neurona $i$ en el patr√≥n $\\mu$.\n",
    "- $n$ es el n√∫mero de neuronas.\n",
    "\n",
    "---\n",
    "\n",
    "## Propiedades Fundamentales\n",
    "\n",
    "- La funci√≥n de energ√≠a disminuye (o permanece constante) con cada actualizaci√≥n, garantizando convergencia a un m√≠nimo local.\n",
    "- Los m√≠nimos locales de la energ√≠a corresponden a los **patrones almacenados** (o a distorsiones cercanas).\n",
    "- Puede funcionar como **memoria asociativa**: dada una entrada incompleta o ruidosa, la red converge al patr√≥n m√°s parecido almacenado.\n",
    "\n",
    "---\n",
    "\n",
    "## Limitaciones\n",
    "\n",
    "- Capacidad m√°xima limitada a aproximadamente $0.138n$ patrones para una red de $n$ neuronas (por el an√°lisis estad√≠stico de Amit, Gutfreund y Sompolinsky, 1985).\n",
    "- Puede tener m√≠nimos locales espurios (falsos recuerdos).\n",
    "- Solo funciona bien con patrones poco correlacionados entre s√≠.\n",
    "\n",
    "---\n",
    "\n",
    "## Aplicaciones T√≠picas\n",
    "\n",
    "- Memoria asociativa y correcci√≥n de errores.\n",
    "- Reconocimiento de patrones incompletos o ruidosos.\n",
    "- Optimizaci√≥n combinatoria (relacionada con el problema del viajero).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"conection.png\" width=\"4000\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"hapi.png\" width=\"4000\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"network hapi.png\" width=\"4000\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"energy.png\" width=\"4000\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"mini.png\" width=\"4000\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"hebian.png\" width=\"4000\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejemplo Completo: Almacenamiento y Recuperaci√≥n de un Patr√≥n en una Red de Hopfield\n",
    "\n",
    "Supongamos que queremos almacenar un √∫nico patr√≥n \\([+1, -1, +1]\\) en una red de Hopfield con 3 neuronas.\n",
    "\n",
    "### 1. Etapa de Aprendizaje: Almacenamiento del Patr√≥n\n",
    "\n",
    "El objetivo es calcular los pesos $w_{ij}$ para que este patr√≥n se almacene como un estado estable en la red.\n",
    "\n",
    "#### Regla de aprendizaje de Hebb:\n",
    "$$\n",
    "w_{ij} = \\frac{1}{N} \\sum_{\\mu=1}^{P} \\xi_{i\\mu} \\xi_{j\\mu}, \\quad w_{ii} = 0,\n",
    "$$\n",
    "donde:\n",
    "\n",
    "- $N$ es el n√∫mero de neuronas.\n",
    "- $\\xi_{i\\mu}$ es el estado de la neurona $i$ en el patr√≥n $\\mu$.\n",
    "- $P$ es el n√∫mero de patrones almacenados (en este caso, 1).\n",
    "\n",
    "Patr√≥n dado:\n",
    "\n",
    "$$\n",
    "\\xi = [+1, -1, +1].\n",
    "$$\n",
    "\n",
    "#### C√°lculo de los pesos $w_{ij}$: Para cada par de neuronas $i,j$, usamos:\n",
    "\n",
    "$$\n",
    "w_{ij} = \\xi_i \\cdot \\xi_j.\n",
    "$$\n",
    "\n",
    "Construimos la matriz de pesos $W$:\n",
    "\n",
    "$$\n",
    "W = \\begin{bmatrix}\n",
    "w_{11} & w_{12} & w_{13} \\\\\n",
    "w_{21} & w_{22} & w_{23} \\\\\n",
    "w_{31} & w_{32} & w_{33}\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "Como $w_{ii} = 0$ (una neurona no se conecta consigo misma):\n",
    "\n",
    "$$\n",
    "w_{11} = w_{22} = w_{33} = 0.\n",
    "$$\n",
    "\n",
    "Para $w_{12}$:\n",
    "\n",
    "$$\n",
    "w_{12} = \\xi_1 \\cdot \\xi_2 = (+1)(-1) = -1.\n",
    "$$\n",
    "\n",
    "Para $w_{13}$:\n",
    "\n",
    "$$\n",
    "w_{13} = \\xi_1 \\cdot \\xi_3 = (+1)(+1) = +1.\n",
    "$$\n",
    "\n",
    "Para $w_{23}$:\n",
    "\n",
    "$$\n",
    "w_{23} = \\xi_2 \\cdot \\xi_3 = (-1)(+1) = -1.\n",
    "$$\n",
    "\n",
    "Dado que la red es sim√©trica ($w_{ij} = w_{ji}$):\n",
    "\n",
    "$$\n",
    "w_{21} = w_{12}, \\quad w_{31} = w_{13}, \\quad w_{32} = w_{23}.\n",
    "$$\n",
    "\n",
    "Finalmente:\n",
    "\n",
    "$$\n",
    "W = \\begin{bmatrix}\n",
    "0 & -1 & +1 \\\\\n",
    "-1 & 0 & -1 \\\\\n",
    "+1 & -1 & 0\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "Ahora hemos almacenado el patr√≥n \\([+1, -1, +1]\\) en la red.\n",
    "\n",
    "### 2. Etapa de Recuperaci√≥n: Convergencia hacia el Patr√≥n\n",
    "\n",
    "Supongamos que introducimos una entrada inicial incompleta o ruidosa, como \\([+1, +1, -1]\\), y queremos que la red recupere el patr√≥n original \\([+1, -1, +1]\\).\n",
    "\n",
    "#### Din√°mica de actualizaci√≥n:\n",
    "La regla para actualizar una neurona $i$ es:\n",
    "\n",
    "$$\n",
    "s_i(t+1) = \n",
    "\\begin{cases}\n",
    "+1 & \\text{si } \\sum_{j} w_{ij} s_j(t) > 0, \\\\\n",
    "-1 & \\text{si } \\sum_{j} w_{ij} s_j(t) < 0.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "**Paso 1: Calcular el estado de cada neurona**\n",
    "\n",
    "Estado inicial:\n",
    "\n",
    "$$\n",
    "s = [+1, +1, -1].\n",
    "$$\n",
    "\n",
    "Actualizar $s_1$: La entrada a la neurona 1 es:\n",
    "\n",
    "$$\n",
    "\\sum_{j} w_{1j} s_j = w_{11} s_1 + w_{12} s_2 + w_{13} s_3 = 0 \\cdot (+1) + (-1) \\cdot (+1) + 1 \\cdot (-1) = -2.\n",
    "$$\n",
    "\n",
    "Como la entrada es negativa, actualizamos $s_1 = -1$.\n",
    "\n",
    "Actualizar $s_2$: La entrada a la neurona 2 es:\n",
    "\n",
    "$$\n",
    "\\sum_{j} w_{2j} s_j = w_{21} s_1 + w_{22} s_2 + w_{23} s_3 = (-1) \\cdot (+1) + 0 \\cdot (+1) + (-1) \\cdot (-1) = 0.\n",
    "$$\n",
    "\n",
    "Como la entrada es cero, $s_2$ permanece igual.\n",
    "\n",
    "Actualizar $s_3$: La entrada a la neurona 3 es:\n",
    "\n",
    "$$\n",
    "\\sum_{j} w_{3j} s_j = w_{31} s_1 + w_{32} s_2 + w_{33} s_3 = 1 \\cdot (+1) + (-1) \\cdot (+1) + 0 \\cdot (-1) = 0.\n",
    "$$\n",
    "\n",
    "Como la entrada es cero, $s_3$ permanece igual.\n",
    "\n",
    "El nuevo estado es:\n",
    "\n",
    "$$\n",
    "s = [-1, +1, -1].\n",
    "$$\n",
    "\n",
    "**Paso 2: Continuar con la actualizaci√≥n hasta que la red converja.**\n",
    "\n",
    "El proceso contin√∫a hasta que todas las neuronas converjan al patr√≥n almacenado. En este caso, despu√©s de algunas iteraciones, la red recuperar√° el patr√≥n \\([+1, -1, +1]\\)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMplementacion de una Hopfield Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Definici√≥n de la clase de la Red de Hopfield\n",
    "class HopfieldNetwork:\n",
    "    def __init__(self, n_units):\n",
    "        # N√∫mero de neuronas (dimensi√≥n del patr√≥n a memorizar)\n",
    "        self.n_units = n_units\n",
    "        # Inicializamos la matriz de pesos como una matriz de ceros\n",
    "        self.weights = np.zeros((n_units, n_units))\n",
    "\n",
    "    def train(self, patterns):\n",
    "        # Entrena la red mediante la regla de Hebb\n",
    "        for p in patterns:\n",
    "            # Aseguramos que el patr√≥n tenga forma columna (n, 1)\n",
    "            p = p.reshape(-1, 1)\n",
    "            # Actualizamos la matriz de pesos: W += p * p^T\n",
    "            self.weights += p @ p.T\n",
    "        # Quitamos las autoconexiones (peso de la neurona i a s√≠ misma = 0)\n",
    "        np.fill_diagonal(self.weights, 0)\n",
    "        # Normalizamos por la cantidad de patrones\n",
    "        self.weights /= len(patterns)\n",
    "\n",
    "    def energy(self, state):\n",
    "        # Calcula la energ√≠a del sistema para un estado dado\n",
    "        # La energ√≠a baja cuando el patr√≥n es uno almacenado\n",
    "        return -0.5 * state @ self.weights @ state.T\n",
    "\n",
    "    def recall(self, pattern, steps=10):\n",
    "        # Recupera un patr√≥n a partir de una entrada ruidosa o incompleta\n",
    "        state = pattern.copy()\n",
    "        for _ in range(steps):\n",
    "            # Se actualiza cada neurona una por una (sincr√≥nicamente)\n",
    "            for i in range(self.n_units):\n",
    "                # Producto punto de la fila i de pesos por el estado actual\n",
    "                raw = np.dot(self.weights[i], state)\n",
    "                # Regla de activaci√≥n: signo del potencial (Hopfield binario -1 o 1)\n",
    "                state[i] = 1 if raw >= 0 else -1\n",
    "        # Retorna el patr√≥n actualizado tras las iteraciones\n",
    "        return state\n",
    "\n",
    "\n",
    "# Creamos patrones simples de ejemplo (5x5 p√≠xeles -> 25 neuronas)\n",
    "def plot_pattern(pattern, title=\"\"):\n",
    "    # Dibuja el patr√≥n como una imagen\n",
    "    plt.imshow(pattern.reshape(5, 5), cmap=\"binary\")\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Definimos dos patrones binarios de 5x5 (convertidos en vectores de -1 y 1)\n",
    "p1 = np.array([1, 1, 1, 1, 1,\n",
    "               1, -1, -1, -1, 1,\n",
    "               1, -1, 1, -1, 1,\n",
    "               1, -1, -1, -1, 1,\n",
    "               1, 1, 1, 1, 1])\n",
    "\n",
    "p2 = np.array([-1, -1, 1, -1, -1,\n",
    "               -1, 1, 1, 1, -1,\n",
    "               1, -1, 1, -1, 1,\n",
    "               -1, 1, 1, 1, -1,\n",
    "               -1, -1, 1, -1, -1])\n",
    "\n",
    "# Instanciamos la red con 25 neuronas\n",
    "net = HopfieldNetwork(n_units=25)\n",
    "\n",
    "# Entrenamos la red con los dos patrones\n",
    "net.train([p1, p2])\n",
    "\n",
    "# Visualizamos los patrones entrenados\n",
    "plot_pattern(p1, \"Pattern 1\")\n",
    "plot_pattern(p2, \"Pattern 2\")\n",
    "\n",
    "# Creamos un patr√≥n ruidoso (perturbamos p1)\n",
    "noisy_p1 = p1.copy()\n",
    "noisy_p1[0] = -1\n",
    "noisy_p1[24] = -1\n",
    "\n",
    "# Creamos un patr√≥n ruidoso (perturbamos p2)\n",
    "noisy_p2 = p2.copy()\n",
    "noisy_p2[0] = 1\n",
    "noisy_p2[24] =1\n",
    "\n",
    "plot_pattern(noisy_p1, \"Noisy Pattern 1\")\n",
    "\n",
    "plot_pattern(noisy_p2, \"Noisy Pattern 1\")\n",
    "# Recuperamos el patr√≥n original a partir del ruidoso\n",
    "recalled = net.recall(noisy_p1, steps=5)\n",
    "recalled2 = net.recall(noisy_p2, steps=5)\n",
    "plot_pattern(recalled, \"Recalled Pattern from Noisy Input\")\n",
    "plot_pattern(recalled2, \"Recalled Pattern from Noisy Input\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# - 4.2 [Boltzman Machine](#activation-functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"boltz1.png\" width=\"4000\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"boltzman2.png\" width=\"4000\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"boltz3.png\" width=\"4000\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"boltz4.png\" width=\"4000\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"boltz5.png\" width=\"4000\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"boltz6.png\" width=\"4000\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"boltz8.png\" width=\"4000\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"boltz9.png\" width=\"4000\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"boltz10.png\" width=\"4000\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"boltz12.png\" width=\"4000\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"boltz13.png\" width=\"4000\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"boltz14.png\" width=\"4000\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"boltz15.png\" width=\"4000\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"boltz16.png\" width=\"4000\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"boltz17.png\" width=\"4000\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"boltz18.png\" width=\"4000\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"boltz19.png\" width=\"4000\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"boltz20.png\" width=\"4000\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M√°quinas de Boltzmann (Boltzmann Machines)\n",
    "\n",
    "Las **M√°quinas de Boltzmann** son un tipo de red neuronal estoc√°stica recurrente que extiende las ideas de las **Redes de Hopfield** al introducir **variables aleatorias** y un modelo basado en la estad√≠stica de sistemas f√≠sicos. Fueron introducidas por **Geoffrey Hinton y Terry Sejnowski en 1985**.\n",
    "\n",
    "Estas redes est√°n inspiradas en principios de la **f√≠sica estad√≠stica**, espec√≠ficamente en los modelos de **Ising** y la **distribuci√≥n de Boltzmann**. Son utilizadas para aprender distribuciones de probabilidad complejas sobre datos y para resolver problemas de optimizaci√≥n dif√≠ciles.\n",
    "\n",
    "---\n",
    "\n",
    "## Estructura de una Boltzmann Machine\n",
    "\n",
    "### Componentes:\n",
    "- **Unidades visibles ($v$):** Corresponden a los datos observables de entrada.\n",
    "- **Unidades ocultas ($h$):** Modelan dependencias latentes o no observables.\n",
    "- Todas las neuronas est√°n conectadas entre s√≠ de forma sim√©trica.\n",
    "- Los estados posibles son usualmente **binarios**: $s_i \\in \\{0, 1\\}$.\n",
    "\n",
    "---\n",
    "\n",
    "## Funci√≥n de Energ√≠a\n",
    "\n",
    "La m√°quina de Boltzmann define una **funci√≥n de energ√≠a** para cualquier configuraci√≥n de unidades visibles e invisibles $(v, h)$:\n",
    "\n",
    "$$\n",
    "E(v, h) = -\\sum_{i<j} w_{ij} s_i s_j - \\sum_{i} b_i s_i\n",
    "$$\n",
    "Donde:\n",
    "- $w_{ij}$ son los pesos sin√°pticos (sim√©tricos: $w_{ij} = w_{ji}$).\n",
    "- $b_i$ es el sesgo (bias) de la neurona $i$.\n",
    "- $s_i$ es el estado binario de la neurona $i$.\n",
    "\n",
    "---\n",
    "\n",
    "## Distribuci√≥n de Boltzmann\n",
    "\n",
    "La probabilidad de una configuraci√≥n $(v, h)$ viene dada por la **distribuci√≥n de Boltzmann**:\n",
    "\n",
    "$$\n",
    "P(v, h) = \\frac{1}{Z} e^{-E(v, h)}\n",
    "$$\n",
    "\n",
    "Donde $Z$ es la constante de normalizaci√≥n conocida como la **partici√≥n**:\n",
    "\n",
    "$$\n",
    "Z = \\sum_{v, h} e^{-E(v, h)}\n",
    "$$\n",
    "\n",
    "Esta distribuci√≥n favorece configuraciones de baja energ√≠a.\n",
    "\n",
    "---\n",
    "\n",
    "## Objetivo del Aprendizaje\n",
    "\n",
    "El objetivo del entrenamiento es ajustar los pesos $w_{ij}$ y sesgos $b_i$ para que la distribuci√≥n marginal sobre las unidades visibles $P(v)$ aproxime la distribuci√≥n de los datos.\n",
    "\n",
    "$$\n",
    "P(v) = \\sum_h P(v, h)\n",
    "$$\n",
    "\n",
    "La funci√≥n de costo se define como la **divergencia Kullback-Leibler (KL)** entre la distribuci√≥n de los datos y la distribuci√≥n del modelo.\n",
    "\n",
    "---\n",
    "\n",
    "## Algoritmo de Aprendizaje\n",
    "\n",
    "El gradiente del logaritmo de la probabilidad est√° dado por:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\log P(v)}{\\partial w_{ij}} = \\langle s_i s_j \\rangle_{\\text{datos}} - \\langle s_i s_j \\rangle_{\\text{modelo}}\n",
    "$$\n",
    "\n",
    "- $\\langle \\cdot \\rangle_{\\text{datos}}$: esperanza sobre las muestras reales.\n",
    "- $\\langle \\cdot \\rangle_{\\text{modelo}}$: esperanza sobre las muestras generadas por la m√°quina.\n",
    "\n",
    "**Dificultad:** Calcular $\\langle \\cdot \\rangle_{\\text{modelo}}$ requiere simular toda la red hasta alcanzar el equilibrio, lo que es computacionalmente costoso.\n",
    "\n",
    "---\n",
    "\n",
    "## M√°quinas de Boltzmann Restringidas (RBM)\n",
    "\n",
    "Para simplificar el aprendizaje, se introducen las **Restricted Boltzmann Machines (RBM)**, donde:\n",
    "- No hay conexiones entre unidades visibles.\n",
    "- No hay conexiones entre unidades ocultas.\n",
    "\n",
    "Esto permite que las unidades ocultas sean **condicionalmente independientes** dadas las visibles, y viceversa.\n",
    "\n",
    "---\n",
    "\n",
    "## Aplicaciones T√≠picas\n",
    "\n",
    "- Reducci√≥n de dimensionalidad.\n",
    "- Detecci√≥n de caracter√≠sticas latentes.\n",
    "- Pre-entrenamiento no supervisado en redes profundas (Deep Belief Networks).\n",
    "- Modelado generativo de datos.\n",
    "\n",
    "---\n",
    "\n",
    "## Referencias Clave\n",
    "\n",
    "- Hinton, G. E., & Sejnowski, T. J. (1985). A Learning Algorithm for Boltzmann Machines. *Cognitive Science*, 9(1), 147‚Äì169.\n",
    "- Hinton, G. E. (2002). Training products of experts by minimizing contrastive divergence. *Neural Computation, 14*(8), 1771‚Äì1800.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. [Pysics Informed Neural Network](#convolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"a.png\" width=\"4000\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Physics-Informed Neural Networks (PINNs)\n",
    "\n",
    "## ¬øQu√© son las PINNs?\n",
    "\n",
    "Las **Physics-Informed Neural Networks (PINNs)** son una clase de redes neuronales dise√±adas para resolver problemas gobernados por ecuaciones diferenciales (ordinarias o parciales), incorporando directamente el conocimiento f√≠sico (las ecuaciones) dentro del proceso de entrenamiento.\n",
    "\n",
    "La idea central es que, adem√°s de ajustar los datos, la red respete las leyes f√≠sicas representadas por las ecuaciones diferenciales. Esta t√©cnica fue popularizada por **M. Raissi, P. Perdikaris y G. E. Karniadakis** en 2019.\n",
    "\n",
    "---\n",
    "\n",
    "## ¬øC√≥mo funcionan las PINNs?\n",
    "\n",
    "### 1Ô∏è‚É£ Aproximaci√≥n con Redes Neuronales\n",
    "Sea $u(x, t)$ la soluci√≥n buscada de una EDP o EDO. Se aproxima mediante una red neuronal:\n",
    "$$\n",
    "u_{\\theta}(x, t) \\approx u(x, t)\n",
    "$$\n",
    "donde $\\theta$ son los par√°metros (pesos y sesgos) de la red.\n",
    "\n",
    "### 2Ô∏è‚É£ Incorporaci√≥n de la F√≠sica (Ecuaciones Diferenciales)\n",
    "Supongamos que la ecuaci√≥n que gobierna el sistema es:\n",
    "$$\n",
    "\\mathcal{N}[u](x, t) = 0\n",
    "$$\n",
    "donde $\\mathcal{N}$ es un operador diferencial (puede incluir derivadas espaciales, temporales, no linealidades, etc.).\n",
    "\n",
    "La red se entrena para que **$u_{\\theta}(x, t)$ no solo ajuste los datos conocidos, sino que tambi√©n satisfaga esta ecuaci√≥n diferencial.**\n",
    "\n",
    "---\n",
    "\n",
    "## Funci√≥n de P√©rdida (Loss Function)\n",
    "\n",
    "La funci√≥n de p√©rdida t√≠pica combina dos t√©rminos:\n",
    "\n",
    "### üîπ **P√©rdida de datos (Data Loss):**\n",
    "Ajusta los datos disponibles (condiciones iniciales, de frontera, observaciones).\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{data}} = \\frac{1}{N_d} \\sum_{i=1}^{N_d} |u_{\\theta}(x_i, t_i) - u_i^{\\text{data}}|^2\n",
    "$$\n",
    "\n",
    "### üîπ **P√©rdida f√≠sica (Physics Loss):**\n",
    "Asegura que la red cumpla la ecuaci√≥n diferencial en puntos seleccionados dentro del dominio.\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{physics}} = \\frac{1}{N_f} \\sum_{i=1}^{N_f} |\\mathcal{N}[u_{\\theta}](x_i, t_i)|^2\n",
    "$$\n",
    "\n",
    "### üîπ **P√©rdida total:**\n",
    "$$\n",
    "\\mathcal{L} = \\mathcal{L}_{\\text{data}} + \\lambda \\mathcal{L}_{\\text{physics}}\n",
    "$$\n",
    "donde $\\lambda$ pondera la importancia relativa de las condiciones f√≠sicas respecto a los datos.\n",
    "\n",
    "---\n",
    "\n",
    "## ¬øC√≥mo se calcula $\\mathcal{N}[u_{\\theta}]$?\n",
    "Las PINNs usan **diferenciaci√≥n autom√°tica** (autodiff) para calcular derivadas de la red respecto a sus entradas, lo que permite construir $\\mathcal{N}[u_{\\theta}]$ sin necesidad de discretizar.\n",
    "\n",
    "Por ejemplo:\n",
    "```python\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x)\n",
    "    u = model(x)\n",
    "    u_x = tape.gradient(u, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"pinns1.png\" width=\"4000\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"pinns2.png\" width=\"4000\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"pinns3.png\" width=\"4000\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"pinns4.png\" width=\"4000\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"pinns5.png\" width=\"4000\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"pinns6.png\" width=\"4000\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"pinns7.png\" width=\"4000\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"pinns8.png\" width=\"4000\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Codigo ejemplo para el problema de la ecuacion de DKP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ecuaci√≥n de Duffin-Kemmer-Petiau (DKP)\n",
    "\n",
    "## Introducci√≥n\n",
    "\n",
    "La **Ecuaci√≥n de Duffin-Kemmer-Petiau (DKP)** es una ecuaci√≥n relativista que describe part√≠culas **bos√≥nicas** (esp√≠n 0 y esp√≠n 1), an√°loga a la ecuaci√≥n de Dirac, que est√° formulada para fermiones (esp√≠n 1/2). Fue propuesta independientemente por **R.J. Duffin (1938), N. Kemmer (1939) y G. Petiau (1936)**.\n",
    "\n",
    "La formulaci√≥n DKP unifica, dentro de una misma estructura algebraica matricial, las ecuaciones de **Klein-Gordon** (esp√≠n 0) y **Proca** (esp√≠n 1), mediante matrices que satisfacen una algebra espec√≠fica.\n",
    "\n",
    "---\n",
    "\n",
    "## Forma General de la Ecuaci√≥n DKP\n",
    "\n",
    "La ecuaci√≥n DKP se escribe como:\n",
    "\n",
    "$$\n",
    "(i \\beta^{\\mu} \\partial_{\\mu} - m) \\psi = 0\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "- $\\psi$ es el campo DKP, un vector de dimensi√≥n 5 (para esp√≠n 0) o 10 (para esp√≠n 1).\n",
    "- $\\beta^{\\mu}$ son matrices que obedecen la **√°lgebra DKP**.\n",
    "- $m$ es la masa de la part√≠cula.\n",
    "- $\\partial_{\\mu}$ es la derivada parcial covariante.\n",
    "\n",
    "---\n",
    "\n",
    "## √Ålgebra DKP\n",
    "\n",
    "Las matrices $\\beta^{\\mu}$ satisfacen las siguientes relaciones:\n",
    "\n",
    "$$\n",
    "\\beta^{\\mu} \\beta^{\\nu} \\beta^{\\lambda} + \\beta^{\\lambda} \\beta^{\\nu} \\beta^{\\mu} = \\beta^{\\mu} \\eta^{\\nu \\lambda} + \\beta^{\\lambda} \\eta^{\\nu \\mu}\n",
    "$$\n",
    "\n",
    "Donde $\\eta^{\\mu\\nu}$ es la m√©trica de Minkowski, t√≠picamente $\\text{diag}(1, -1, -1, -1)$.\n",
    "\n",
    "Esta √°lgebra es diferente a la del √°lgebra de Clifford que satisface la ecuaci√≥n de Dirac.\n",
    "\n",
    "---\n",
    "\n",
    "## Representaciones\n",
    "- **Representaci√≥n de 5x5 matrices**: describe part√≠culas de **esp√≠n 0** (equivalente a Klein-Gordon).\n",
    "- **Representaci√≥n de 10x10 matrices**: describe part√≠culas de **esp√≠n 1** (equivalente a Proca).\n",
    "\n",
    "---\n",
    "\n",
    "## Lagrangiano\n",
    "\n",
    "El lagrangiano que genera la ecuaci√≥n DKP por principio variacional es:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\bar{\\psi} (i \\beta^{\\mu} \\partial_{\\mu} - m) \\psi\n",
    "$$\n",
    "donde $\\bar{\\psi} = \\psi^{\\dagger} \\eta$ y $\\eta$ es una matriz definida para que la teor√≠a sea invariante bajo transformaciones de Lorentz y que asegure un lagrangiano herm√≠tico.\n",
    "\n",
    "---\n",
    "\n",
    "## Propiedades Generales\n",
    "\n",
    "- La ecuaci√≥n DKP es **lineal en derivadas**, igual que la de Dirac.\n",
    "- Describe **campos bos√≥nicos** (no fermiones).\n",
    "- Puede acoplarse de forma natural a campos electromagn√©ticos mediante la derivada covariante:\n",
    "$$\n",
    "\\partial_{\\mu} \\rightarrow D_{\\mu} = \\partial_{\\mu} + i e A_{\\mu}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Relaci√≥n con otras ecuaciones\n",
    "\n",
    "| Tipo de Campo | Ecuaci√≥n Relacionada | Dimensi√≥n DKP | \n",
    "|---------------|----------------------|---------------|\n",
    "| Escalar       | Klein-Gordon          | 5             |\n",
    "| Vectorial     | Proca                 | 10            |\n",
    "\n",
    "---\n",
    "\n",
    "## Corriente Conservada\n",
    "\n",
    "Se puede definir una corriente conservada en DKP como:\n",
    "$$\n",
    "j^{\\mu} = \\bar{\\psi} \\beta^{\\mu} \\psi\n",
    "$$\n",
    "satisfaciendo $\\partial_{\\mu} j^{\\mu} = 0$.\n",
    "\n",
    "---\n",
    "\n",
    "## Diferencias con Dirac\n",
    "| Caracter√≠stica     | Dirac            | DKP               |\n",
    "|--------------------|------------------|-------------------|\n",
    "| Tipo de part√≠cula  | Fermiones (esp√≠n 1/2) | Bosones (esp√≠n 0, 1) |\n",
    "| √Ålgebra            | Clifford         | DKP               |\n",
    "| Representaci√≥n     | 4x4               | 5x5 o 10x10        |\n",
    "| Combinatoria algebraica | Anticonmutadores | Relaci√≥n trilineal |\n",
    "\n",
    "---\n",
    "\n",
    "## Referencias Fundamentales\n",
    "\n",
    "- Duffin, R.J. \"On the Characteristic Matrices of Covariant Systems.\" *Phys. Rev.* **54**, 1114 (1938).\n",
    "- Kemmer, N. \"The Particle Aspect of Meson Theory.\" *Proc. Roy. Soc. A* **173**, 91 (1939).\n",
    "- Petiau, G. \"On Dirac-type Equations for Bosons.\" *Acad. Roy. de Belg., Cl. Sci. Mem. Collect.*, 16(2), 1936.\n",
    "- Lunardi, J. T., Manzoni, L. A., Pimentel, B. M., & Teixeira, R. G. (2000). \"Remarks on the Duffin-Kemmer-Petiau theory and gauge invariance.\" *International Journal of Theoretical Physics*, 39(1), 121-131.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ecuaci√≥n tipo Klein-Gordon a resolver (Potencial hiperb√≥lico tangente)\n",
    "\n",
    "## Forma general de la ecuaci√≥n\n",
    "\n",
    "Cuando se trabaja con la representaci√≥n escalar del formalismo **Duffin-Kemmer-Petiau (DKP) para esp√≠n 0**, la ecuaci√≥n se reduce a una ecuaci√≥n tipo **Klein-Gordon** con potencial escalar o vectorial, dependiendo del acoplamiento. En el art√≠culo estudiado, la ecuaci√≥n a resolver es la siguiente:\n",
    "\n",
    "$$\n",
    "\\frac{d^2 \\phi(x)}{dx^2} + \\left\\{ [E - V(x)]^2 - m^2 \\right\\} \\phi(x) = 0\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "- $ E $ es la energ√≠a de la part√≠cula.\n",
    "- $ V(x) $ es el potencial aplicado.\n",
    "- $ m $ es la masa de la part√≠cula.\n",
    "- $ \\phi(x) $ es la funci√≥n de onda escalar.\n",
    "\n",
    "---\n",
    "\n",
    "## Potencial utilizado: Potencial hiperb√≥lico tangente\n",
    "\n",
    "El potencial considerado en este caso es el **potencial hiperb√≥lico tangente**, definido como:\n",
    "\n",
    "$$\n",
    "V(x) = a \\tanh(bx)\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "- $ a $ controla la altura del potencial.\n",
    "- $ b $ controla la suavidad de la transici√≥n.\n",
    "\n",
    "Este potencial tiene la siguiente forma cualitativa:\n",
    "- Para $ x \\to -\\infty $, $ V(x) \\to -a $\n",
    "- Para $ x \\to +\\infty$, $ V(x) \\to a $\n",
    "\n",
    "Cuando $b \\to \\infty$, el potencial se aproxima a un escal√≥n.\n",
    "\n",
    "---\n",
    "\n",
    "## Ecuaci√≥n espec√≠fica a resolver\n",
    "\n",
    "Al reemplazar el potencial en la ecuaci√≥n de Klein-Gordon, la ecuaci√≥n concreta a resolver es:\n",
    "\n",
    "$$\n",
    "\\frac{d^2 \\phi(x)}{dx^2} + \\left\\{ [E - a \\tanh(bx)]^2 - m^2 \\right\\} \\phi(x) = 0\n",
    "$$\n",
    "\n",
    "Esta es una **ecuaci√≥n diferencial de segundo orden no lineal en la variable dependiente del potencial**.\n",
    "\n",
    "---\n",
    "\n",
    "## Cambio de variable sugerido en el art√≠culo\n",
    "\n",
    "El art√≠culo propone el cambio:\n",
    "$$\n",
    "y = - e^{2bx}\n",
    "$$\n",
    "Este cambio transforma la ecuaci√≥n en una forma que permite reducirla a la **ecuaci√≥n hipergeom√©trica** cl√°sica.\n",
    "\n",
    "La forma intermedia obtenida es:\n",
    "$$\n",
    "4b^2 y \\frac{d}{dy} \\left( y \\frac{d\\phi}{dy} \\right) + \\left[ \\left( \\frac{E + a}{1 + y} - \\frac{E - a}{1 - y} \\right)^2 - m^2 \\right] \\phi(y) = 0\n",
    "$$\n",
    "\n",
    "Finalmente, mediante un ansatz del tipo:\n",
    "$$\n",
    "\\phi(y) = y^\\alpha (1 - y)^\\beta f(y)\n",
    "$$\n",
    "la ecuaci√≥n se reduce a una **ecuaci√≥n hipergeom√©trica** para \\( f(y) \\).\n",
    "\n",
    "---\n",
    "\n",
    "## Soluci√≥n general\n",
    "\n",
    "La soluci√≥n general se expresa en t√©rminos de funciones hipergeom√©tricas:\n",
    "$$\n",
    "\\phi(x) = c_1 (-e^{2bx})^{i\\nu} (1 + e^{2bx})^\\lambda \\, {}_2F_1( \\ldots ) + c_2 (-e^{2bx})^{-i\\nu} (1 + e^{2bx})^\\lambda \\, {}_2F_1( \\ldots )\n",
    "$$\n",
    "\n",
    "Con los par√°metros definidos como:\n",
    "$$\n",
    "\\nu = \\frac{\\sqrt{(E + a)^2 - m^2}}{2b}, \\quad \\mu = \\frac{\\sqrt{(E - a)^2 - m^2}}{2b}, \\quad \\lambda = \\frac{b + \\sqrt{b^2 - 4a^2}}{2b}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Interpretaci√≥n f√≠sica\n",
    "\n",
    "- En el l√≠mite $ x \\to \\pm \\infty $, la ecuaci√≥n describe ondas planas libres, permitiendo definir coeficientes de reflexi√≥n \\( R \\) y transmisi√≥n \\( T \\).\n",
    "- El fen√≥meno de **superradiancia** (cuando $ R > 1 $) ocurre para ciertos rangos de energ√≠a.\n",
    "\n",
    "---\n",
    "\n",
    "## Referencia principal del art√≠culo\n",
    "\n",
    "**Clara Rojas (2018)**  \n",
    "Scattering of a scalar relativistic particle by the hyperbolic tangent potential.  \n",
    "https://arxiv.org/abs/1409.6342v1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ecuaci√≥n espec√≠fica a resolver\n",
    "\n",
    "Al reemplazar el potencial en la ecuaci√≥n de Klein-Gordon, la ecuaci√≥n concreta a resolver es:\n",
    "\n",
    "$$\n",
    "\\frac{d^2 \\phi(x)}{dx^2} + \\left\\{ [E - a \\tanh(bx)]^2 - m^2 \\right\\} \\phi(x) = 0\n",
    "$$\n",
    "\n",
    "Esta es una **ecuaci√≥n diferencial de segundo orden no lineal en la variable dependiente del potencial**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "#matplotlib.use('TkAgg') \n",
    "import matplotlib.pyplot as plt\n",
    "import sympy as sp\n",
    "from PIL import Image\n",
    "from IPython.display import display, Image\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from scipy.optimize import fsolve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Par√°metros de la ecuaci√≥n de Klein-Gordon\n",
    "E = 8 # Energ√≠a\n",
    "a = 5.0  # Amplitud de la barrera\n",
    "b = 2.0  # Pendiente de la tangente hiperb√≥lica\n",
    "m = 1.0  # Masa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k(E, a, b, m, x):\n",
    "    x = tf.cast(x, tf.float32)  # üîÅ Asegura tipo compatible\n",
    "    R = (E - a * tf.math.tanh(b * x))**2 - m**2\n",
    "    return tf.sqrt(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir la funci√≥n k usando numpy en lugar de TensorFlow para compatibilidad con fsolve\n",
    "def k_numpy(E, a, b, m, x):\n",
    "    R = (E - a * np.tanh(b * x))**2 - m**2\n",
    "    return np.sqrt(R)\n",
    "\n",
    "# Funci√≥n cuyo cero queremos encontrar\n",
    "def f(x, E, a, b, m):\n",
    "    return x + (5 * np.pi / k_numpy(E, a, b, m, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suposici√≥n inicial\n",
    "x_inicial = -1.0\n",
    "# Resolver\n",
    "x_sol = fsolve(f, x_inicial, args=(E, a, b, m))[0]\n",
    "\n",
    "print(f\"Soluci√≥n encontrada: x = {x_sol}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0=-10#x_sol-1\n",
    "x01=-10#x_sol-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k(E, a, b, m, x0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dominio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usamos una √∫nica posici√≥n de frontera:\n",
    "x_bc1 = np.linspace(-x01, x01-0.1, 250)[:, None]\n",
    "x_left = np.linspace(x01, -3, 1000)[:, None]\n",
    "x_left = tf.convert_to_tensor(x_left, dtype=tf.float32)  # Aseguramos que x_left sea un tensor tf.float32\n",
    "# 2. Dominio interno (sin incluir frontera izquierda)\n",
    "x_domain_np = np.linspace(x01, -x01, 1000)[:, None]  # ‚Üê Este es el dominio de entrenamiento para la PDE\n",
    "x_domain = tf.convert_to_tensor(x_domain_np, dtype=tf.float32)\n",
    "\n",
    "# Dominio derecho (para la frontera derecha)\n",
    "x_right1 = np.linspace(0,-x01, 1000)[:, None]\n",
    "x_right = tf.convert_to_tensor(x_right1, dtype=tf.float32)  # Aseguramos que x_left sea un tensor tf.float32\n",
    "\n",
    "\n",
    "x_left1 = np.linspace(x01, 0, 1000)[:, None]\n",
    "x_left1 = tf.convert_to_tensor(x_left1, dtype=tf.float32)  # Aseguramos que x_left sea un tensor tf.float32\n",
    "# Dominio de evaluaci√≥n\n",
    "x_test_np = np.linspace(x01, -x01, 1000)[:, None]  # 500 puntos uniformemente distribuidos\n",
    "x_test = tf.convert_to_tensor(x_test_np, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_val = k(E, a, b, m, x0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MOdelos usados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sin_activation(x):\n",
    "    return tf.math.sin(x)\n",
    "\n",
    "def cos_activation(x):\n",
    "    return tf.math.cos(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model1():\n",
    "    model1 = {\n",
    "        'dense1': tf.keras.layers.Dense(20, activation=sin_activation),\n",
    "        'dense2': tf.keras.layers.Dense(50, activation=cos_activation),\n",
    "        'dense3': tf.keras.layers.Dense(20, activation=sin_activation),\n",
    "        'dense4': tf.keras.layers.Dense(50, activation=cos_activation),\n",
    "        'output_layer': tf.keras.layers.Dense(1)\n",
    "    }\n",
    "    return model1\n",
    "\n",
    "def call_model1(model1, x):\n",
    "    x = model1['dense1'](x)\n",
    "    x = model1['dense2'](x)\n",
    "    x = model1['dense3'](x)\n",
    "    x = model1['dense4'](x)\n",
    "    x = model1['output_layer'](x)\n",
    "    return x\n",
    "\n",
    "def create_model2():\n",
    "    model2 = {\n",
    "        'dense1': tf.keras.layers.Dense(20, activation=sin_activation),\n",
    "        'dense2': tf.keras.layers.Dense(50, activation=cos_activation),\n",
    "        'dense3': tf.keras.layers.Dense(20, activation=sin_activation),\n",
    "        'dense4': tf.keras.layers.Dense(50, activation=cos_activation),\n",
    "        'output_layer': tf.keras.layers.Dense(1)\n",
    "    }\n",
    "    return model2\n",
    "\n",
    "def call_model2(model2, x):\n",
    "    x = model2['dense1'](x)\n",
    "    x = model2['dense2'](x)\n",
    "    x = model2['dense3'](x)\n",
    "    x = model2['dense4'](x)\n",
    "    x = model2['output_layer'](x)\n",
    "    return x\n",
    "\n",
    "def create_model3():\n",
    "    model3 = {\n",
    "        'dense1': tf.keras.layers.Dense(20, activation=sin_activation),\n",
    "        'dense2': tf.keras.layers.Dense(50, activation=cos_activation),\n",
    "        'dense3': tf.keras.layers.Dense(20, activation=sin_activation),\n",
    "        'dense4': tf.keras.layers.Dense(50, activation=cos_activation),\n",
    "        'output_layer': tf.keras.layers.Dense(1)\n",
    "    }\n",
    "    return model3\n",
    "\n",
    "def call_model3(model3, x):\n",
    "    x = model3['dense1'](x)\n",
    "    x = model3['dense2'](x)\n",
    "    x = model3['dense3'](x)\n",
    "    x = model3['dense4'](x)\n",
    "    x = model3['output_layer'](x)\n",
    "    return x\n",
    "\n",
    "def create_model4():\n",
    "    model4 = {\n",
    "        'dense1': tf.keras.layers.Dense(20, activation=sin_activation),\n",
    "        'dense2': tf.keras.layers.Dense(50, activation=cos_activation),\n",
    "        'dense3': tf.keras.layers.Dense(20, activation=sin_activation),\n",
    "        'dense4': tf.keras.layers.Dense(50, activation=cos_activation),\n",
    "        'output_layer': tf.keras.layers.Dense(1)\n",
    "    }\n",
    "    return model4\n",
    "\n",
    "def call_model4(model4, x):\n",
    "    x = model4['dense1'](x)\n",
    "    x = model4['dense2'](x)\n",
    "    x = model4['dense3'](x)\n",
    "    x = model4['dense4'](x)\n",
    "    x = model4['output_layer'](x)\n",
    "    return x\n",
    "\n",
    "def create_model5():\n",
    "    model5 = {\n",
    "        'dense1': tf.keras.layers.Dense(20, activation=sin_activation),\n",
    "        'dense2': tf.keras.layers.Dense(50, activation=cos_activation),\n",
    "        'dense3': tf.keras.layers.Dense(20, activation=sin_activation),\n",
    "        'dense4': tf.keras.layers.Dense(50, activation=cos_activation),\n",
    "        'output_layer': tf.keras.layers.Dense(1)\n",
    "    }\n",
    "    return model5\n",
    "\n",
    "def call_model5(model5, x):\n",
    "    x = model5['dense1'](x)\n",
    "    x = model5['dense2'](x)\n",
    "    x = model5['dense3'](x)\n",
    "    x = model5['dense4'](x)\n",
    "    x = model5['output_layer'](x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Total Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_loss(model1,model2,model3,model4,model5, x, x_left,x_right):\n",
    "    \n",
    "   \n",
    " \n",
    "    \n",
    "\n",
    "    with tf.GradientTape(persistent=True) as tape2:\n",
    "        tape2.watch(x)\n",
    "        with tf.GradientTape(persistent=True) as tape1:\n",
    "            tape1.watch(x)\n",
    "            f1 = call_model1(model1, x)\n",
    "            f2 = call_model2(model2, x)\n",
    "            #f3 = call_model3(model3, x)\n",
    "            #f4 = call_model4(model4, x)\n",
    "            #f5 = call_model5(model5, x)\n",
    "\n",
    "        df1_dx = tape1.gradient(f1, x)\n",
    "        df2_dx = tape1.gradient(f2, x)\n",
    "        #df3_dx = tape1.gradient(f3, x)\n",
    "        #df4_dx = tape1.gradient(f4, x)\n",
    "        #df5_dx = tape1.gradient(f5, x)\n",
    "        \n",
    "    d2f1_dx2 = tape2.gradient(df1_dx, x)\n",
    "    d2f2_dx2 = tape2.gradient(df2_dx, x)\n",
    "    #d2f3_dx2 = tape2.gradient(df3_dx, x)\n",
    "    #d2f4_dx2 = tape2.gradient(df4_dx, x)\n",
    "    #d2f5_dx2 = tape2.gradient(df5_dx, x)\n",
    "    \n",
    "\n",
    "    V = (E - a * tf.tanh(b * x))**2 - m**2\n",
    "\n",
    "    pde1_residual = d2f1_dx2 + V * f1\n",
    "    pde2_residual = d2f2_dx2 + V * f2\n",
    "    #pde3_residual = d2f3_dx2 + V * f3\n",
    "    #pde4_residual = d2f4_dx2 + V * f4\n",
    "    #pde5_residual = d2f5_dx2 + V * f5\n",
    "     \n",
    "\n",
    "    # Restricciones en x_left (solo aqu√≠)\n",
    "    with tf.GradientTape(persistent=True) as tape_left:\n",
    "        tape_left.watch(x_left)\n",
    "        f1_left = call_model1(model1, x_left)\n",
    "        #f2_left = call_model2(model2, x_left)\n",
    "       # f3_left = call_model3(model3, x_left)\n",
    "        #f4_left = call_model4(model4, x_left)\n",
    "        #f5_left = call_model5(model5, x_left)\n",
    "        \n",
    "    df1_dx_left = tape_left.gradient(f1_left, x_left)\n",
    "    #df2_dx_left = tape_left.gradient(f2_left, x_left)\n",
    "    #df3_dx_left = tape_left.gradient(f3_left, x_left)\n",
    "    #df4_dx_left = tape_left.gradient(f4_left, x_left)\n",
    "    #df5_dx_left = tape_left.gradient(f5_left, x_left)\n",
    "    \n",
    "    # Restricciones en x_left (solo aqu√≠)\n",
    "    with tf.GradientTape(persistent=True) as tape_right:\n",
    "        tape_right.watch(x_right)\n",
    "        f1_right = call_model1(model1, x_right)\n",
    "        #f2_right = call_model2(model2, x_right)\n",
    "        #f3_right = call_model3(model3, x_right)\n",
    "        #f4_right = call_model4(model4, x_right)\n",
    "        #f5_right = call_model5(model5, x_right)\n",
    "        \n",
    "    df1_dx_right = tape_right.gradient(f1_right, x_right)\n",
    "    #df2_dx_right = tape_right.gradient(f2_right, x_right)\n",
    "    #df3_dx_right = tape_right.gradient(f3_right, x_right)\n",
    "    #df4_dx_right = tape_right.gradient(f4_right, x_right)\n",
    "    #df5_dx_right = tape_right.gradient(f5_right, x_right)   \n",
    "\n",
    "    # Razones\n",
    "    \n",
    "    \n",
    "   \n",
    "    f1_max_left = tf.reduce_max(tf.abs(f1_left))\n",
    "    f1_max_right = tf.reduce_max(tf.abs(f1_right))\n",
    "    #f2_max_left = tf.reduce_max(tf.abs(f2_left))\n",
    "    #f2_max_right = tf.reduce_max(tf.abs(f2_right))\n",
    "    #f3_max_left = tf.reduce_max(tf.abs(f3_left))\n",
    "    #f3_max_right = tf.reduce_max(tf.abs(f3_right))\n",
    "    #f4_max_left = tf.reduce_max(tf.abs(f4_left))\n",
    "    #f4_max_right = tf.reduce_max(tf.abs(f4_right))\n",
    "    #f5_max_left = tf.reduce_max(tf.abs(f5_left))\n",
    "    #f5_max_right = tf.reduce_max(tf.abs(f5_right))\n",
    "    razon1= f1_max_right / f1_max_left\n",
    "    #razon2= f2_max_right / f2_max_left\n",
    "    #razon3= f3_max_right / f3_max_left\n",
    "    #razon4= f4_max_right / f4_max_left\n",
    "    #razon5= f5_max_right / f5_max_left\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Restricciones en x_right (solo aqu√≠)\n",
    "    with tf.GradientTape(persistent=True) as tape_right:\n",
    "        tape_right.watch(x_right)\n",
    "        f1_right = call_model1(model1, x_right)\n",
    "        \n",
    "    df1_dx_right = tape_right.gradient(f1_right, x_right)\n",
    "    \n",
    "    # Onda reflejada como diferencia\n",
    "    Onda_reflejada = f1_right - f1_left\n",
    "\n",
    "    Onda_Incidente1=1*tf.math.cos(k_val * x_left)+1*tf.math.sin(k_val * x_left)+1*tf.math.cos(k_val * x_left)-1*tf.math.sin(k_val * x_left)\n",
    "    Derivada_Onda_Incidente1 = -1*k_val * tf.math.sin(k_val * x_left)+1*k_val * tf.math.cos(k_val * x_left)-1*k_val * tf.math.sin(k_val * x_left)-1*k_val * tf.math.cos(k_val * x_left)\n",
    "    \n",
    "    #Onda_Incidente2=2*tf.math.cos(k_val * x_left)+2*tf.math.sin(k_val * x_left)\n",
    "    #Derivada_Onda_Incidente2 = -2*k_val * tf.math.sin(k_val * x_left)+2*k_val * tf.math.cos(k_val * x_left)\n",
    "    #Onda_Incidente3=3*tf.math.cos(k_val * x_left)-3*tf.math.sin(k_val * x_left)\n",
    "    #Derivada_Onda_Incidente3 = -3*k_val * tf.math.sin(k_val * x_left)-3*k_val * tf.math.cos(k_val * x_left)\n",
    "    #Onda_Incidente4=4*tf.math.cos(k_val * x_left)-4*tf.math.sin(k_val * x_left)\n",
    "    #Derivada_Onda_Incidente4 = -4*k_val * tf.math.sin(k_val * x_left)-4*k_val * tf.math.cos(k_val * x_left)\n",
    "    #Onda_Incidente5=5*tf.math.cos(k_val * x_left)-5*tf.math.sin(k_val * x_left)\n",
    "    #Derivada_Onda_Incidente5 = -5*k_val * tf.math.sin(k_val * x_left)-5*k_val * tf.math.cos(k_val * x_left)\n",
    "\n",
    "\n",
    "    constraint_1= f1_left-Onda_Incidente1\n",
    "    constraint_2= df1_dx_left - Derivada_Onda_Incidente1\n",
    "\n",
    "    #constraint_3= f2_left-Onda_Incidente2\n",
    "    #constraint_4= df2_dx_left - Derivada_Onda_Incidente2\n",
    "    #constraint_5= f3_left-Onda_Incidente3\n",
    "    #constraint_6= df3_dx_left - Derivada_Onda_Incidente3\n",
    "    #constraint_7= f4_left-Onda_Incidente4\n",
    "    #constraint_8= df4_dx_left - Derivada_Onda_Incidente4\n",
    "    #constraint_9= f5_left-Onda_Incidente5\n",
    "    #constraint_10= df5_dx_left - Derivada_Onda_Incidente5\n",
    "    \n",
    "    \n",
    "    # Normas L2 para amplitudes en cada regi√≥n\n",
    "    f1_norm_left = tf.sqrt(tf.reduce_mean(tf.square(f1_left)))\n",
    "    reflejada_norm_left = tf.sqrt(tf.reduce_mean(tf.square(Onda_reflejada)))\n",
    "    f_total_right = f1_right \n",
    "    f_total_norm_right = tf.sqrt(tf.reduce_mean(tf.square(f_total_right)))\n",
    "\n",
    "    # N√∫meros de onda\n",
    "    k_vals_right = k(E, a, b, m, -x0)\n",
    "    k_vals_right = tf.where(tf.math.is_nan(k_vals_right), tf.zeros_like(k_vals_right), k_vals_right)\n",
    "    k_vals_left = k(E, a, b, m, x0)\n",
    "    k_vals_left = tf.where(tf.math.is_nan(k_vals_left), tf.zeros_like(k_vals_left), k_vals_left)\n",
    "    k_max_val_right = tf.reduce_max(k_vals_right)\n",
    "    k_max_val_left = tf.reduce_max(k_vals_left)\n",
    "\n",
    "    # Coeficientes\n",
    "    #R = (reflejada_norm_left / f1_norm_left)**2\n",
    "    #T = (k_max_val_right / k_max_val_left) * (f_total_norm_right / f1_norm_left)**2 \n",
    "\n",
    "    \n",
    "\n",
    "   \n",
    "\n",
    "    \n",
    "\n",
    "   \n",
    "    \n",
    "\n",
    "\n",
    "    loss1 = (\n",
    "        tf.reduce_mean(tf.square(pde1_residual)) +\n",
    "        \n",
    "        \n",
    "        1e2*tf.reduce_mean(tf.square(constraint_1))  +\n",
    "        1e2*tf.reduce_mean(tf.square(constraint_2))  \n",
    "        \n",
    "    )\n",
    "    #loss2 = (\n",
    "        #tf.reduce_mean(tf.square(pde2_residual)) +\n",
    "        \n",
    "        \n",
    "        #1e2*tf.reduce_mean(tf.square(constraint_3))  +\n",
    "        #1e2*tf.reduce_mean(tf.square(constraint_4))  \n",
    "        \n",
    "    #)\n",
    "    return loss1,razon1\n",
    "'''\n",
    "    loss2 = (\n",
    "        \n",
    "        tf.reduce_mean(tf.square(pde2_residual)) +\n",
    "        \n",
    "        \n",
    "        \n",
    "        1e2*tf.reduce_mean(tf.square(constraint_3))  +\n",
    "        1e2*tf.reduce_mean(tf.square(constraint_4))  \n",
    "        \n",
    "    )\n",
    "\n",
    "    loss3 = (\n",
    "        \n",
    "        tf.reduce_mean(tf.square(pde3_residual)) +\n",
    "        \n",
    "          +\n",
    "        1e2*tf.reduce_mean(tf.square(constraint_5))  +\n",
    "        1e2*tf.reduce_mean(tf.square(constraint_6))  \n",
    "        \n",
    "          \n",
    "    )\n",
    "\n",
    "    loss4 = (\n",
    "        \n",
    "        tf.reduce_mean(tf.square(pde4_residual)) +\n",
    "        \n",
    "        \n",
    "        \n",
    "        1e2*tf.reduce_mean(tf.square(constraint_7))  +\n",
    "        1e2*tf.reduce_mean(tf.square(constraint_8))  \n",
    "        \n",
    "    )\n",
    "\n",
    "    loss5 = (\n",
    "        \n",
    "        tf.reduce_mean(tf.square(pde5_residual)) +\n",
    "        \n",
    "        \n",
    "        1e2*tf.reduce_mean(tf.square(constraint_9))  +\n",
    "        1e2*tf.reduce_mean(tf.square(constraint_10)) \n",
    "          \n",
    "    )\n",
    "    '''\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traing step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "\n",
    "def train_step_total_loss(model1,model2,model3,model4,model5, x_domain,x_left, x_right, optimizer1, optimizer2, optimizer3, optimizer4, optimizer5):\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        loss1,razon1 = total_loss(model1,model2,model3,model4,model5, x_domain,x_left, x_right)\n",
    "    \n",
    "    # Obtener las variables entrenables de cada modelo (asumiendo que son diccionarios de capas)\n",
    "    variables1 = [var for layer in model1.values() for var in layer.trainable_variables]\n",
    "    #variables2 = [var for layer in model2.values() for var in layer.trainable_variables]\n",
    "    #variables3 = [var for layer in model3.values() for var in layer.trainable_variables]    \n",
    "    #variables4 = [var for layer in model4.values() for var in layer.trainable_variables]\n",
    "    #variables5 = [var for layer in model5.values() for var in layer.trainable_variables]\n",
    "\n",
    "    # Calcular gradientes de loss respecto a variables de cada modelo\n",
    "    grads1 = tape.gradient(loss1, variables1)\n",
    "    #grads2 = tape.gradient(loss2, variables2)\n",
    "    #grads3 = tape.gradient(loss3, variables3)\n",
    "    #grads4 = tape.gradient(loss4, variables4)\n",
    "    #grads5 = tape.gradient(loss5, variables5)\n",
    "   \n",
    "\n",
    "    # Aplicar gradientes\n",
    "    optimizer1.apply_gradients(zip(grads1, variables1))\n",
    "    #optimizer2.apply_gradients(zip(grads2, variables2))\n",
    "    #optimizer3.apply_gradients(zip(grads3, variables3))\n",
    "    #optimizer4.apply_gradients(zip(grads4, variables4))\n",
    "    #optimizer5.apply_gradients(zip(grads5, variables5))\n",
    "   \n",
    "\n",
    "    del tape\n",
    "    return loss1,razon1#,loss3,loss4,loss5,razon1,razon2,razon3,razon4,razon5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calling the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear modelos\n",
    "model1 = create_model1()\n",
    "model2 = create_model2()\n",
    "model3 = create_model3()\n",
    "model4 = create_model4()\n",
    "model5 = create_model5()\n",
    "\n",
    "# Historial de predicciones\n",
    "history_y_pred_1 = []\n",
    "history_y_pred_2 = []\n",
    "#history_y_pred_3 = []\n",
    "#history_y_pred_4 = []\n",
    "#history_y_pred_5 = []\n",
    "\n",
    "history_loss1 = []\n",
    "history_loss2 = []\n",
    "#history_loss3 = []\n",
    "#history_loss4 = []\n",
    "#history_loss5 = []\n",
    "f10_history = []\n",
    "f20_history = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule1 = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.001,\n",
    "    decay_steps=1000,\n",
    "    decay_rate=0.9\n",
    ")\n",
    "lr_schedule2 = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.001,\n",
    "    decay_steps=1000,\n",
    "    decay_rate=0.9\n",
    ")\n",
    "lr_schedule3 = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.001,\n",
    "    decay_steps=1000,\n",
    "    decay_rate=0.9\n",
    ")\n",
    "lr_schedule4 = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.001,\n",
    "    decay_steps=1000,\n",
    "    decay_rate=0.9\n",
    ")\n",
    "lr_schedule5 = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.001,\n",
    "    decay_steps=1000,\n",
    "    decay_rate=0.9\n",
    ")\n",
    "\n",
    "\n",
    "optimizer1 = tf.keras.optimizers.Adam(learning_rate=lr_schedule1)\n",
    "optimizer2 = tf.keras.optimizers.Adam(learning_rate=lr_schedule2)\n",
    "optimizer3 = tf.keras.optimizers.Adam(learning_rate=lr_schedule3)\n",
    "optimizer4 = tf.keras.optimizers.Adam(learning_rate=lr_schedule4)\n",
    "optimizer5 = tf.keras.optimizers.Adam(learning_rate=lr_schedule5)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Hiperpar√°metros\n",
    "epochs = 500000\n",
    "\n",
    "\n",
    "plt.ion()\n",
    "threshold = 1e-4\n",
    "max_epochs = 535000  # por seguridad\n",
    "epoch = 0\n",
    "loss_val = tf.constant(1.0)\n",
    "\n",
    "while loss_val.numpy() > threshold and epoch < max_epochs:\n",
    "#fig, ax = plt.subplots(figsize=(10, 6))  # Una sola figura y eje\n",
    "# Entrenamiento conjunto con p√©rdida acoplada\n",
    "#for epoch in range(epochs):\n",
    "    loss_val,razon1=train_step_total_loss(model1,model2,model3,model4,model5, x_domain,x_left, x_right, optimizer1,optimizer2, optimizer3, optimizer4, optimizer5)\n",
    "    x_0 = tf.constant([[0.0]], dtype=tf.float32)\n",
    "    x_0 = tf.Variable(x_0)  # Variable para gradiente\n",
    "\n",
    "    def calcular_valor_y_derivada(model, call_model, x):\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(x)\n",
    "            y = call_model(model, x)\n",
    "        dy_dx = tape.gradient(y, x)\n",
    "        return y, dy_dx\n",
    "     # Calcular para cada modelo\n",
    "    f10, df1_dx0 = calcular_valor_y_derivada(model1, call_model1, x_0)\n",
    "    #f20, df2_dx0 = calcular_valor_y_derivada(model2, call_model2, x_0)\n",
    "    f10_val = f10.numpy().item()\n",
    "    df1_dx0_val = df1_dx0.numpy().item()\n",
    "    #f20_val = f20.numpy().item()\n",
    "    #df2_dx0_val = df2_dx0.numpy().item()\n",
    "    history_loss1.append(loss_val.numpy())\n",
    "    #history_loss2.append(loss2.numpy())\n",
    "    #history_loss3.append(loss3.numpy())\n",
    "    #history_loss4.append(loss4.numpy())\n",
    "    #history_loss5.append(loss5.numpy())\n",
    "    # Calcular coeficientes\n",
    "    \n",
    "\n",
    "    \n",
    "    # Guardar predicciones actuales\n",
    "    y_pred_1 = call_model1(model1, x_test).numpy()\n",
    "    #y_pred_2= call_model2(model2, x_test).numpy()\n",
    "    #y_pred_3 = call_model3(model3, x_test).numpy()\n",
    "    #y_pred_4 = call_model4(model4, x_test).numpy()\n",
    "    #y_pred_5 = call_model5(model5, x_test).numpy()\n",
    "\n",
    "    #y_pred_left = call_model1(model1, x_left1).numpy()\n",
    "    #y_pred_right = call_model1(model1, x_right).numpy()\n",
    "    history_y_pred_1.append( y_pred_1)\n",
    "    #history_y_pred_2.append( y_pred_2)\n",
    "    #history_y_pred_3.append( y_pred_3)\n",
    "    #history_y_pred_4.append( y_pred_4)\n",
    "    #history_y_pred_5.append( y_pred_5) \n",
    "\n",
    "\n",
    "    #history_y_pred_2.append( y_pred_left)\n",
    "    #history_y_pred_3.append( y_pred_right)\n",
    "\n",
    "    #y_combined =  history_y_pred_1[-1].flatten() +  history_y_pred_2[-1].flatten()\n",
    "\n",
    "\n",
    "    if epoch % 1500 == 0:\n",
    "        print(\n",
    "            f\"Epoch {epoch}/{epochs} | \"\n",
    "            f\"Total Loss: {loss_val:.4e} | \"\n",
    "            f\"razon1: {razon1:.4e} | \"\n",
    "           #f\"razon2: {razon2:.4e} | \"\n",
    "            #f\"razon3: {razon3:.4e} | \"\n",
    "            #f\"razon4: {razon4:.4e} | \"\n",
    "            #f\"razon5: {razon5:.4e}  \"\n",
    "           \n",
    "            \n",
    "      )\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        start_idx = max(0, epoch - 2500)\n",
    "        colors1 = plt.cm.viridis(np.linspace(0, 1, len(history_y_pred_1)))\n",
    "       \n",
    "\n",
    "        for count, i in enumerate(range(start_idx, epoch + 1, 500)):\n",
    "            plt.plot(x_test, history_y_pred_1[i], color='blue', linestyle='--', \n",
    "                     label=(f'Onda Incidente - Epoch {i},R1={razon1:.2e},\\n'f'N1(0)={f10_val:.2e}, dN1/dx(0)={df1_dx0_val:.2e}'if count == 0 else \"\"), alpha=0.6)\n",
    "           # plt.plot(x_test, history_y_pred_2[i], color='red', linestyle='--',\n",
    "                        #label=(f'Onda Relejada - Epoch {i}R2={razon2:.2e},\\n'f'N1(0)={f20_val:.2e}, dN1/dx(0)={df2_dx0_val:.2e}'if count == 0 else \"\"), alpha=0.6)\n",
    "            #plt.plot(x_test, y_combined, label=r'Solucion Onda Incidente + Onda Reflejada'if count == 0 else \"\", alpha=0.6, color='green', linewidth=2)\n",
    "            #plt.plot(x_test, history_y_pred_3[i], color='green', linestyle='--',\n",
    "                        #label=f'Amplitud3 - Epoch {i}' if count == 0 else \"\", alpha=0.6)\n",
    "            #plt.plot(x_test, history_y_pred_4[i], color='orange', linestyle='--',\n",
    "                        #label=f'Amplitud4 - Epoch {i}' if count == 0 else \"\", alpha=0.6)\n",
    "            #plt.plot(x_test, history_y_pred_5[i], color='purple', linestyle='--',\n",
    "                        #label=f'Amplitud5 - Epoch {i}' if count == 0 else \"\", alpha=0.6)\n",
    "            # L√≠nea original\n",
    "            #plt.plot(x_left1, history_y_pred_2[i], color='red', linestyle='--', \n",
    "                     #label=f'Onda Incidente - Epoch {i}' if count == 0 else \"\", alpha=0.6)\n",
    "            #plt.plot(x_right, history_y_pred_3[i], color='green', linestyle='--', \n",
    "                     #label=f'Onda Trasmitida - Epoch {i}' if count == 0 else \"\", alpha=0.6)\n",
    "            #plt.plot(x_left1, history_y_pred_3[i]-history_y_pred_2[i],color='black', linestyle='--', label=f'œà Onda reflejada- Epoch {i}' if count == 0 else \"\", alpha=0.6)\n",
    "        # Graficar la combinaci√≥n lineal\n",
    "            \n",
    "        plt.xlabel(\"x\")\n",
    "        plt.ylabel(\"y_pred\")\n",
    "        plt.title(f\"Soluciones comparadas hasta epoch {epoch}\")\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.pause(0.01)\n",
    "        plt.clf()\n",
    "        #fig.canvas.draw()\n",
    "        #fig.canvas.flush_events()\n",
    "    epoch += 1  # Importante para evitar bucle infinito   \n",
    "# Finalizar modo interactivo\n",
    "plt.ioff()\n",
    "\n",
    "# Gr√°fico final de evoluci√≥n\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i in range(0, len(history_y_pred_1), 500):\n",
    "    plt.plot(x_test, history_y_pred_1[i], 'b--', label=f'Onda Incidente - Epoch {i}', alpha=0.6)\n",
    "    #plt.plot(x_left, Onda_reflejada, label=\"œà reflejada\", linestyle=\"--\")\n",
    "\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y_pred\")\n",
    "plt.title(\"Evoluci√≥n final \")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
